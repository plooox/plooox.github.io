[{"categories":["troubleshooting"],"content":"Springboot 프로젝트 중 발생한 Trouble 해결 과정","date":"2022-04-19","objectID":"/troubleshooting_springjpa/","tags":["springboot"],"title":"[Trouble Shooting] JPA에서 Like, Contains구문 사용시 에러","uri":"/troubleshooting_springjpa/"},{"categories":["troubleshooting"],"content":"React + Springboot + Mysql을 사용한 프로젝트를 진행하던 중 발생한 문제를 다룹니다. 문제상황 React.useEffect()를 사용해 웹 페이지에 접속과 동시에, api 서버 요청을 통해 특정 문자열이 포함된 데이터를 DB에서 가져오는 기능을 구현하고 있었습니다. JpaRepository를 사용하여 Mysql DB서버에 특정 문자열이 포함된 DB를 사용하고자 다음과 같은 Service 코드를 작성하였습니다. // Repository package com.example.modoosugang_be.Repository; import com.example.modoosugang_be.Domain.Lecture; import org.springframework.data.jpa.repository.JpaRepository; import org.springframework.data.jpa.repository.Query; import org.springframework.data.repository.query.Param; import java.util.List; public interface LectureRepository extends JpaRepository\u003cLecture, Long\u003e { List\u003cLecture\u003e findAllByProfessorContains(String professor); } // Service package com.example.modoosugang_be.Service; import com.example.modoosugang_be.Domain.Lecture; import com.example.modoosugang_be.Repository.LectureRepository; import lombok.RequiredArgsConstructor; import org.springframework.stereotype.Service; import java.util.List; @Service @RequiredArgsConstructor public class LectureService { private final LectureRepository lectureRepository; public List\u003cLecture\u003ecallUnivLecture(String univ) { List\u003cLecture\u003e lectures = lectureRepository.findAllByProfessorContains(univ); return lectures; } } 원래대로라면 Jpa를 통해 명명된 Method들이 알아서 잘 작동되어야 하는데, 웬걸? 이상한 문제상황에 직면했다. 찾아보니 JPA사용 중 java.lang.IllegalArgumentException 은 Entity 작성을 잘못하거나 하면 종종 발생하는 에러라고 한다. 그런데 내 경우에는 특이하게도 서버를 돌리는 첫 시행에는 돌아가다가, 반복 시행시 에러가 나는 기이한(?) 현상이 발생하였다. 원인분석 + 해결 에러를 해결하기 위해 여러 가지 시도를 해봤었다. Entity \u0026 DB 매칭 재확인 Jpa에서 contains말고 startWith, like 사용해보기 DB에 Column을 추가해서 contains말고 findAllBy만 사용하기 시도해보니 3번 방법말고는 해결이되지 않았었다… 진짜로 DB를 엎어야되나 생각하던 중, 이런 걸 확인해볼 수 있었다. 내 경우와는 똑같은 것은 아니었지만 위 Issue를 보니, Springboot 최신버전과 Hibernate를 같이 사용시 startingWith, contains, startsWith, Like와 같은 구문을 사용시 동일한 에러를 발생시킨다는 걸 알 수 있었다. Participants들의 코멘트들을 보니, Hibernate버전을 다운그레이드 하거나, @Query를 이용해 직접 쿼리문을 작성하면 해결되는 것을 확인할 수 있었다. 팀 단위로 굴러가는 프로젝트다보니, 버전을 수정하는 것보다 직접 쿼리문을 작성하는 방법을 선택했다. // ~중략~ public interface LectureRepository extends JpaRepository\u003cLecture, Long\u003e { @Query(value = \"SELECT v FROM Lecture v WHERE v.professor Like :univ%\") List\u003cLecture\u003e findLecture(@Param(\"univ\")String univ); } 평소에 에러가 발생하면 구글이나 Stackoverflow만 검색했었는데, Github도 유심히 봐야겠다. ","date":"2022-04-19","objectID":"/troubleshooting_springjpa/:0:0","tags":["springboot"],"title":"[Trouble Shooting] JPA에서 Like, Contains구문 사용시 에러","uri":"/troubleshooting_springjpa/"},{"categories":["project"],"content":"k8s 전문가 양성 과정 중 진행하게된 프로젝트","date":"2022-04-17","objectID":"/frontend/","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":" Goorm kubernetes 전문가 양성 과정 중 프로젝트 진행 과정을 공유합니다. Front-end: UI 구현 React js를 통해 UI를 구현했습니다. 팀원 모두가 React사용 경험이 없어 개인공부를 어느정도로 실시한 다음에 구현을 진행하였습니다. 저는 생활코딩으로 기초적인 지식을 쌓았습니다. 강의하나만으로 구현할 수 있는 정도까지 실력을 쌓았으면 좋았겠지만, 아무리 UI 디자인을 크게 신경쓰지 않는다고 해도 뚝딱 만들기에는 어려움이 많았습니다. 그래도 구글링 신공(?)으로 리엑트 템플릿이나 다른 사람이 많든 코드들을 보며 하나둘씩 UI 컴포넌트들을 만들다보니, 어떻게 만들어야할지 그래도 감이 잡혔습니다. ","date":"2022-04-17","objectID":"/frontend/:0:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"mui mui라는 React UI 툴을 많이 사용했는데, 이걸 사용하니 그래도 봐줄만한 UI가 완성되었습니다. ","date":"2022-04-17","objectID":"/frontend/:1:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"react-table react-table은 React로 Table UI를 간단하게 구현할 수 있도록 해주는 라이브러리로, 테이블 내 데이터를 검색하거나 정렬하는 것을 지원해주기 때문에 좀 더 쉽게 UI를 구현할 수 있었습니다. 회고 처음 시작할 때 프론트엔드는 못할거 같다는 생각이 들었는데, 그래도 막상 만들다보니 어느정도 감이 잡히고, 해볼만하다는 생각이 들었습니다. Backend쪽을 희망해서 프론트엔드는 등한시 했었는데, 서로 긴밀하게 연결되는 분야라 나중에 BE를 공부할 때도 많은 도움이 될거 같다는 생각이 들었습니다.(물론 backend를 해본건 아니지만… 😭) UI 구현하면서 Github로 협업하는 것도 처음 해보았는데 Organization Repo에서 fork해와서 작업하고, Github branch전략에 따라 기능별로 branch노 나누어서 구현 해보고, Issue\u0026PR도 생성해보면서 add, commit, push밖에 모르던 git을 좀 더 잘 쓸수 있게 된거 같습니다 ㅎㅎ. ","date":"2022-04-17","objectID":"/frontend/:2:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"k8s 전문가 양성 과정 중 진행하게된 프로젝트","date":"2022-04-03","objectID":"/design/","tags":["k8s","AWS"],"title":"[k8s project] 주제 정하기 ~ 설계","uri":"/design/"},{"categories":["project"],"content":" Goorm kubernetes 전문가 양성 과정 중 프로젝트 진행 과정을 공유합니다. 프로젝트 시작 교육과정이 전부 끝나고, 팀프로젝트가 시작되었습니다. 어쩌다보니 팀장이 되어버렸는데 이왕 하는거 열심히 해보려고 합니다. 🔥🔥🔥 주제 정하기 아무래도 교육과정 동안 배웠던 내용이 k8s가 주가 되었으니, 이것을 적극적으로 활용할만한 주제가 무엇이 있는지 팀원들과 의논해보았습니다. k8s를 활용한 다른 사람들의 프로젝트들을 찾아본 결과, k8s의 HPA를 이용하여 서버 부하에 따라 적절하게 Scale in/out하는 프로젝트들을 볼 수 있었습니다. (참고) 저의 팀도 Auto Scaling을 통해 서버 부하를 원활하게 대응하는 것을 보여줄 수 있는 프로젝트를 해보면 어떨까라는 이야기가 오갔고, 특정 기간에만 서버 부하가 집중되는 서비스가 어떤게 있을까 고민해본 결과 수강신청 사이트를 k8s를 통해 구현해보면 좋겠다는 생각을 했습니다. 아이디어 구체화(초안) 기술스택 교육과정동안 배운 내용을 바탕으로 + $\\alpha$ 하여 사용할 기술을 선정하였습니다. 설계 (아키텍쳐 \u0026 UI) 4명의 팀원들이 2명씩 나누어서 아키텍쳐 설계와 UI설계를 진행하였습니다. 저는 아키텍쳐 설계를 담당하게 되었습니다. ","date":"2022-04-03","objectID":"/design/:0:0","tags":["k8s","AWS"],"title":"[k8s project] 주제 정하기 ~ 설계","uri":"/design/"},{"categories":["project"],"content":"아키텍쳐 설계 DevOps 과정을 이수했다보니 설계에 있어서 AWS 클라우드 환경과 CI/CD 신경을 많이 썼습니다. eks를 사용하여 k8s클러스터 환경을 AWS환경에서 구현하고, 여기에 모니터링/로깅 시스템을 덧붙이고, Jenkins/ArgoCD로 빌드, 배포환경을 조성하기로 계획하였습니다. 설계를 해보는 것이 이번이 처음이라 머리가 빠개질것같았지만, 설계를 하고나니 프로젝트를 어떻게 진행하면 될지 윤곽을 잡을 수 있었습니다. ","date":"2022-04-03","objectID":"/design/:1:0","tags":["k8s","AWS"],"title":"[k8s project] 주제 정하기 ~ 설계","uri":"/design/"},{"categories":["project"],"content":"UI 설계 저랑 다른 팀원이 아키텍쳐 설계를 하는 동안, 다른 두 팀원분들께서 웹 UI설계를 해주셨습니다. (링크) 회고 팀을 꾸려 개발 프로젝트를 하는것이 처음이고, 개발 프로젝트 팀장을 맡는 것도 처음이라 부담이 많이 되기는 합니다.😂 그래도 팀장이 되니까 프로젝트에 좀 더 열심히 참여하는거 같고, 배우는것도 더 많이 배우는 것 같아 성취감이 있는거 같습니다. 이제 다음주부터는 FE설계랑 BE설계가 들어갈 예정입니다. 팀 역할을 정할 때 팀원들끼리 역할을 나눌 때 FE/BE 이런식으로 나눈게 아니라 업무별로 나누어 진행을 하기로 정했습니다. 그래서 React와 Springboot 모두 배우면서 진행을 하게 될 예정인데, 많이 바쁠거 같지만 많이 배울 수 있을거 같아 기대됩니다. ","date":"2022-04-03","objectID":"/design/:2:0","tags":["k8s","AWS"],"title":"[k8s project] 주제 정하기 ~ 설계","uri":"/design/"},{"categories":["Algorithm"],"content":"Regular Expression 복잡한 문자열을 처리할 때 사용하는 기법으로, 문자열을 처리하는 모든 곳에서 사용 ","date":"2022-03-08","objectID":"/re/:0:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"메타문자 ","date":"2022-03-08","objectID":"/re/:1:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"[ ] 문자 클래스 [ ] 사이의 문자들과 매치 - : 두 문자 사이의 범위 ^ : not 1. [abc] # a,b,c중 한개의 문자와 매칭 2. [a-c] # == [abc] 3. [a-zA-Z] # 모든 알파벳 4. [0-9] # 숫자 re content \\d 숫자와 매치, [0-9] \\D 숫자가 아닌 것과 매치, [^0-9] \\s whitespace 문자와 매치, [ \\t\\n\\r\\f\\v] \\S whitespace 문자가 아닌것과 매치, [^ \\t\\n\\r\\f\\v] \\w 문자+숫자와 매치, [a-zA-Z0-9_] \\W 문자+숫자가 아닌 것과 매치, [^a-zA-Z0-9_] ","date":"2022-03-08","objectID":"/re/:1:1","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":". 줄바꿈 문자를 제외한 모든 문자 문자 클래스(**[ ]**) 내에 Dot(**.**) 메타 문자가 사용된다면 이것은 “모든 문자\"라는 의미가 아닌 문자 **.** 그대로를 의미 1. a.b # a + [모든 문자] + b 2. a[.]b # a.b ","date":"2022-03-08","objectID":"/re/:1:2","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"*, +, {m,n} 반복 * : 바로 앞에 있는 문자가 0부터 무한대로 반복될 수 있음 + : 바로 앞에 있는 문자가 1부터 무한대로 반복될 수 있음(최소 1번) {m,n}: 바로 앞에 있는 문자가 m부터 n번까지 반복될 수 있음(최소 m번, 최대 n번) ca*t # ct, cat,caat,caaaat, ... ca+t # cat, caat, caaat, ... ca{2}t # caat ca{2,4}t # caat, caaat, caaaat ","date":"2022-03-08","objectID":"/re/:1:3","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"? ’{0, 1}’ 있어도 되고, 없어도 된다 mol?lu # mollu, molu ","date":"2022-03-08","objectID":"/re/:1:4","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"| or ","date":"2022-03-08","objectID":"/re/:1:5","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"^, $,\\A,\\Z ^: 문자열의 맨 처음과 일치, re.MULTILINE사용시 각 줄의 처음과 일치 $: 문자열 맨 끝과 일치, re.MULTILINE사용시 각 줄의 끝과일치 \\A: 전체 문자열의 맨 처음과 일치 \\Z: 전체 문자열의 맨 끝과 일치 ","date":"2022-03-08","objectID":"/re/:1:6","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"\\b, \\B 단어 구분자 \\b: whitespace로 구분된 단어인 경우 매치 \\B: whitespace로 구분된 단어가 아닌 경우 매치 ","date":"2022-03-08","objectID":"/re/:1:7","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"Grouping ( ) group(인덱스) 설명 group(0) 매치된 전체 문자열 group(1) 첫 번째 그룹에 해당되는 문자열 group(2) 두 번째 그룹에 해당되는 문자열 group(n) n 번째 그룹에 해당되는 문자열 ","date":"2022-03-08","objectID":"/re/:2:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"module re 파이썬을 설치할 때 자동으로 설치되는 기본 라이브러리 Method 목적 match() 문자열의 처음부터 정규식과 매치되는지 조사한다. search() 문자열 전체를 검색하여 정규식과 매치되는지 조사한다. findall() 정규식과 매치되는 모든 문자열(substring)을 리스트로 돌려준다. finditer() 정규식과 매치되는 모든 문자열(substring)을 반복 가능한 객체로 돌려준다. # match, search: true면 객체 반환, false면 None 반환 print(p.match(\"python\")) print(p.match(\"3 python\")) print(p.search(\"python\")) print(p.search(\"3 python\")) \u003e\u003e\u003e \u003cre.Match object; span=(0, 6), match='python'\u003e None \u003cre.Match object; span=(0, 6), match='python'\u003e \u003cre.Match object; span=(2, 8), match='python'\u003e -------- # match 객체 활용 \u003e\u003e\u003e m = p.match(\"python\") \u003e\u003e\u003e m.group() # group: 매치된 문자열 반환 'python' \u003e\u003e\u003e m.start() # start: 매치된 문자열의 시작위치 반환 0 \u003e\u003e\u003e m.end() # end: 매치된 문자열의 끝위치 반환 6 \u003e\u003e\u003e m.span() # 매치된 문자열의 (시작, 끝) 반환 (0, 6) print(p.findall(\"life is too short\")) for r in p.finditer(\"life is too short\"): print(r) \u003e\u003e\u003e ['life', 'is', 'too', 'short'] \u003cre.Match object; span=(0, 4), match='life'\u003e \u003cre.Match object; span=(5, 7), match='is'\u003e \u003cre.Match object; span=(8, 11), match='too'\u003e \u003cre.Match object; span=(12, 17), match='short'\u003e ","date":"2022-03-08","objectID":"/re/:3:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"컴파일 옵션 DOTALL(S) - . 이 줄바꿈 문자를 포함하여 모든 문자와 매치할 수 있도록 한다. \u003e\u003e\u003e p = re.compile('a.b', re.DOTALL) \u003e\u003e\u003e m = p.match('a\\nb') \u003e\u003e\u003e print(m) \u003cre.Match object; span=(0, 3), match='a\\nb'\u003e IGNORECASE(I) - 대소문자에 관계없이 매치할 수 있도록 한다. \u003e\u003e\u003e p = re.compile('[a-z]+', re.I) \u003e\u003e\u003e p.match('python') \u003cre.Match object; span=(0, 6), match='python'\u003e \u003e\u003e\u003e p.match('Python') \u003cre.Match object; span=(0, 6), match='Python'\u003e \u003e\u003e\u003e p.match('PYTHON') \u003cre.Match object; span=(0, 6), match='PYTHON'\u003e MULTILINE(M) - 여러줄과 매치할 수 있도록 한다. (^, $ 메타문자의 사용과 관계가 있는 옵션이다) import re p = re.compile(\"^python\\s\\w+\", re.MULTILINE) data = \"\"\"python one life is too short python two you need python python three\"\"\" print(p.findall(data)) \u003e\u003e\u003e ['python one', 'python two', 'python three'] VERBOSE(X) - verbose 모드를 사용할 수 있도록 한다. (정규식을 보기 편하게 만들수 있고 주석등을 사용할 수 있게된다.) charref = re.compile(r\"\"\" \u0026[#] # Start of a numeric entity reference ( 0[0-7]+ # Octal form | [0-9]+ # Decimal form | x[0-9a-fA-F]+ # Hexadecimal form ) ; # Trailing semicolon \"\"\", re.VERBOSE) ","date":"2022-03-08","objectID":"/re/:4:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["Algorithm"],"content":"sub 정규식과 매치되는 부분을 다른 문자로 변경 # [정규식].sub(\"바꿀 문자열\", \"대상 문자열\") \u003e\u003e\u003e p = re.compile('(blue|white|red)') \u003e\u003e\u003e p.sub('colour', 'blue socks and red shoes') 'colour socks and colour shoes' ","date":"2022-03-08","objectID":"/re/:5:0","tags":["Algorithm"],"title":"Regular Expression(정규표현식)","uri":"/re/"},{"categories":["project"],"content":"졸업논문 프로젝트(2)","date":"2022-01-13","objectID":"/3dprinting_printability_ml/","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"https://github.com/plooox/ML_3Dprinter_Scaffold 저번 포스트에서 작성했던 이미지 처리부분에 이어서, 머신러닝 모델 선정, 학습, 분석 과정을 기술합니다. Machine Learning ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:0:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"분류 기반 방법론 작성한 논문에서는 Printability가 높은 파라미터 조합을 얻을 수 있는것을 목표로 하였습니다. 따라서 파라미터 값을 지정해주는 회귀 모델보다 최적 파라미터 조합과 파라미터 범위를 추정할 수 있는 분류 모델이 적합하고 판단하였습니다. 분류 모델은 Support Vector Machine(SVM)과 Random Forest(RF)를 기반으로 하여 두개의 머신러닝 모델을 비교하였습니다. 다음은 표준편차(Std), 전체 픽셀 수(Sum)에 따른 데이터 셋의 분포입니다. 이를 토대로 적절한 임계값을 지정하여 Printability의 “낮음”과 “높음”을 라벨링 하였습니다. 데이터셋의 분포를 “좋음”, “보통”, “나쁨”으로 나누어 3x3 분할된 9개의 Class를 생성하였습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:1:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Train Validation Test 데이터 신뢰성을 위해 train validation test에 기반하여 모델을 평가하였습니다. 학습 세트(train set), 검증 세트(validation set), 평가 세트(test set)에 각각 60:20:20 비율로 데이터를 분할하여 학습시켰습니다. Validation set을 training set으로 hold-out 기반으로 반복 학습시켰고, 가장 잘 학습된 모델을 선정하였습니다. 그리고 학습시킨 model에 test set으로 test해서 최종적인 성능을 측정하고, 학습이 잘 되었는지 분석하는 과정을 거쳤습니다. 본 논문에서는 RF의 하이퍼 파라미터 중 tree개수를 정하는 n_estimators, SVM_RBF의 C, gamma값을 알고리즘의 매개변수로 하여 Test를 통해 최적의 값을 찾아 테스트 하였습니다. 테스트 결과 RF에서는 n_estimator가 25개 이상이 되었을 때, 성능이 크게 증가하는 것을 확인 할 수 있었다. 이를 기반으로 n_estimator값을 61로 설정하고, accuracy_score와 f1_score를 측정해보았습니다. RBF kernel을 사용하는 SVM 알고리즘에서는 정규화 파라미터인 C(cost)와 곡률 파라미터인 Gamma값을 조정하여 최적의 결정 경계를 출력하는 값을 찾고자 하였습니다. 튜닝 결과 gamma = 100, C = 35를 파라미터 값으로 하여 accuracy_score를 측정하였습니다. 결과 및 분석 ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:2:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Accuracy score RF와 SVM의 Accuracy_score를 비교한 표입니다. 두 모델 모두 Validation Set과 Test Set의 차이가 크게 나지 않은 것으로 보아, 학습이 어느정도 일관되게 잘 이루어졌다는 것을 알 수 있었습니다. 그리고 해당 실험에서는 RF모델로 학습을 시켰을 때, 좀 더 높은 정확도가 보이는 것을 알 수 있었습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:3:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Parameter Set 이를 기반으로 RF모델을 통해, 전체 파라미터 범위의 데이터셋을 넣고, 분류해보는 과정을 수행해 보았습니다. 학습된 모델이 데이터셋을 분류하는 것을 확인할 수 있었습니다. 분류한 데이터 셋에서 “좋음”을 나타내는 Class 1~3에 속한 임의의 데이터와 “나쁨을 나타내는 Class 7~9에 속한 임의에 데이터를 추출하여 직접 다시 프린팅 해본 결과, 실제로도 “좋음”과 “나쁨”을 구별할 수 있다는 것을 확인할 수 있었습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:4:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Feature Importance RF모델의 경우, feature_importance_를 통해 매개변수 중요도를 파악할 수 있습니다. 뿐만 아니라 SVM모델에서도 coef_를 통해 중요도를 확인 할 수 있습니다. 참고)https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn 노즐 온도, 출력 속도, 베드 온도 순으로 중요도가 높다는 결과값이 나오게 되었는데, 이처럼 RF모델을 사용하게 된다면 3D 프린터 사용자가 파라미터를 조정시 어떤 파라미터를 주의깊게 조정해야하는지 알 수 있다. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:5:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 머신러닝","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"졸업논문 프로젝트(1)","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"개요 https://github.com/plooox/ML_3Dprinter_Scaffold 졸업논문의 일환으로 팀원들과 “머신러닝을 이용한 3D 프린팅의 printability 최적조건”이라는 주제로 논문을 작성하였습니다. 저는 몇몇 팀원들과 이미지처리부분과 머신러닝 파트를 맡아 진행하였는데, 그 중에서 이미지 처리 과정을 기술하고자 합니다. 논문의 핵심은 3D 프린팅을 할 때 사용되는 여러가지 파라미터를 설정해서 나오는 출력물이 사용자가 원하는 만큼의 품질을 낼 수 있도록 하는 여러 파라미터들의 설정값 조합을 추정하는 과정을 머신러닝을 통해 구할 수 있는지 검증하고, 분석하는 것입니다. 일정 수의 테스트 데이터를 직접 출력하여 머신러닝모델에 학습시키고, 학습된 모델이 추정한 값이 유효한지 검증하였습니다. 이 페이지에서는 아래 빨간 박스인 이미지 처리 부분을 포스트하려고 합니다. 3D printing ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:0:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"도면 도면의 경우, AUTOCAD Inventor프로그램을 사용할 수 있는 팀원을 통해 얇은 1layer로 구성된 500μm 반지름을 가진 작은 원형 pit이 100개 나열된 micro-pit 모델을 설계하였고, 3D printer를 이용해 이 도면을 프린팅 했습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:1:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Parameter selection 실험 설계과정에서 조절하는 파라미터는 출력 속도, 노즐 온도, 베드 온도 총 3가지 파라미터를 선정해 랜덤으로 파라미터 조합을 생성하고 직접 프린트하여 총 278개의 입력 데이터셋을 형성하였습니다. 파라미터 범위는 전체적인 출력 형태를 확인하기 위해 본 실험에서 사용한 3D 프린트의 출력이 불가한 범위를 파악하고 출력이 가능한 범위로 한정하였습니다. Parameter Range 출력 온도 180℃ ~ 240℃ 베드 온도 55℃ ~ 85℃ 출력 속도 30mm/s ~ 150mm/s Image processing 출력한 출력물을 이용해 논문에서 언급한 “좋은 출력물”을 판단할 수 있도록 데이터화 시키는 과정을 거쳤습니다. “좋은 출력물\"은 도면과 유사한 크기의 균일한 공극을 가지는 출력물을 의미합니다. 이를 판단하는 기준인 Printablility는 출력물의 각 공극의 픽셀 합과 핏 간 표준편차를 통해 이루어 졌습니다. 공극의 픽셀 합을 통해 도면과 출력물의 유사성을 판단하고, 표준편차를 통해 균일도를 확인할 수 있기 때문입니다. 따라서 픽셀 합이 크고, 표준편차 값이 낮을수록 Printability가 높다고 정의하였습니다. Printability를 구하기 위해 프린터에서 출력한 이미지를 이미지 처리하는 과정이 이루어졌습니다. 이미지 처리는 Python과 Python 라이브러리인 OpenCV(cv2)를 이용하였습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:2:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Preprocessing 이미지처리 과정을 자동화 하기 위해, 출력물 사진의 바깥쪽 바탕을 제거하고, 흑/백으로 이진화 하는 전처리 과정을 거쳤습니다. 우선, 이미지에서 micro-pits부분만 뽑아내기 위해, Thresh값을 바꾸어가며 이미지를 이진화 해보았습니다. Thresh값이 높아질수록 외부 노이즈는 줄어들지만, 내부 이미지가 잘 나오지 않았고, Thresh값을 낮추면 이미지 자체의 퀄리티는 높지만 외부에 노이즈가 생기는 것을 확인 할 수 있었습니다. 따라서 Thresh값을 높게 잡아 BoundingRect값을 잡았을 때 출력물 테두리가 명확히 잡히도록 하고, 테두리를 기준으로 이미지를 Crop하여 출력물만 나타나는 이미지를 새로 구성한 뒤, 이 이미지를 낮은 Thresh값에서 이진화 하는 방식을 취하였습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:3:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Pixel Counting 이후 Crop된 이미지를 10 x 10으로 구획을 나누어 구획별 검정 픽셀의 수를 구하였고, 이를 통해 전체 이미지에서 검정 픽셀 수, 표준편차를 구하여 데이터셋으로 구축하였습니다. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:4:0","tags":["3D printing","ImageProcessing"],"title":"[Project] 머신러닝을 이용한 3D프린팅의 printability 최적조건 - 이미지 처리","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"NLP Project","date":"2022-01-06","objectID":"/sentimental_analysis_project/","tags":["NLP"],"title":"[Project] SNS 감정분석을 통한 여론 조사","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"개요 https://github.com/plooox/SentimentalAnalysis 대표적인 소셜 네트워크 서비스인 트위터에서 특정 키워드의 검색 결과에 대한 트윗 데이터를 수집하고, 수집한 트윗 내용을 바탕으로 감정 분석을 실시하여 특정 키워드에 대한 여론이 긍정적인지, 부정적인지를 보여주는 프로그램 입니다. 데이터 수집 트위터의 경우, 트위터api를 통해 트위터의 데이터를 받을 수 있습니다. 하지만 트위터 api의 경우, 무료로 사용하게 되면 7일 이내의 데이터만 수집이 가능하다는 단점을 가지고 있습니다. 7일이라는 기간은 내가 필요로 하는 기간(6개월 이상)에 비하면 터무니 없이 부족했기 때문에, 외부 프로그램을 통해 데이터를 크롤링 해야만 했습니다. 오픈소스로 존재하는 수많은 크롤러들 중 대부분은 크롤링이 잘 되지 않았습니다(2021년 8월 기준). 그 중 snscrape라는 파이썬 패키지가 원활하게 데이터 수집이 되는 것을 확인하고, 이를 통해 데이터 수집을 하였습니다. import numpy as np import pandas as pd import datetime as dt import re import pickle import csv import time import snscrape.modules.twitter as sntwitter start = \"2021-01-01\" end = \"2021-08-31\" dateTimeList = [] posNegList = [] contentList = [] tweetList = [] querys = [ #Keywords.... ] for query in querys: params = query+\"lang:ko \"+\"since:\"+start+\" until:\"+end csvRoute = \"./\"+query+\".csv\" csvFile = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") csvWrite.writerow([\"Date\",\"Pos/Neg\",\"Tweet\"]) csvFile.close() # for each keywords... for i,tweet in enumerate(sntwitter.TwitterSearchScraper(params).get_items()): # Twitter data -\u003e Date, Content # Save csv file tw_content = tweet.content tw_date = tweet.date tw_posNeg = -1 #Pos/Neg setting run another code! csvFile = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") csvWrite = csv.writer(csvFile) csvWrite.writerow([tw_date, tw_posNeg, tw_content]) csvFile.close() f = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") r = csv.reader(f) cf = open('./Keyword1.csv', 'a', newline='', encoding='utf-8') wf = csv.writer(cf) wf.writerow([\"Date\",\"Pos/Neg\",\"Tweet\"]) f.close() cf.close() 감정 분석 수집한 트위터 데이터들을 가지고 감정 분석을 하여 트윗 텍스트가 긍정적인 의미를 내포하고 있는지, 부정적인 의미를 내포하고 있는지를 분석합니다. 이를 위해 kakao brain에서 배포한 PORORO(Platform Of neuRal mOdels for natuRal language prOcessing)를 이용하였습니다. PORORO에서는 다양한 자연어 처리 Task를 지원하는데, 이중에서 “Sentimental Analysis”를 이용하면 입력한 텍스트 데이터가 ‘긍정’인지, ‘부정’인지 분류할 수 있습니다. import pandas as pd import numpy as np import csv from pororo import Pororo sa = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") fileList = [#Keywords] # for each keywords.. for keyword in fileList: csvName = keyword+'_sa.csv' csvFile = open(csvName, 'a', newline='', encoding='utf-8') csvWrite = csv.writer(csvFile) csvWrite.writerow(['Date','Pos/Neg','Tweet']) csvFile.close() refCsv = pd.read_csv(keyword+'.csv') for idx, rowData in refCsv.iterrows(): tw_date = rowData['Date'] tw_content = rowData['Tweet'] tw_posneg = -1 try: if sa(tw_content) == \"Positive\": tw_posneg = 1 elif sa(tw_content) == \"Negative\": tw_posneg = 0 # Save csv file with Pos/Neg # Add for each row csvFile = open(csvName, 'a', newline='', encoding='utf-8') csvWrite = csv.writer(csvFile) csvWrite.writerow([tw_date, tw_posneg, tw_content]) csvFile.close() if (idx % 10000 == 0): # Check running print(\"Now: \"+ keyword+ \" - \"+str(idx)) except: print(\"error\") 키워드 추출 이왕 데이터를 수집한 겸, 찾고자 하는 키워드와 연관도가 높은 키워드를 보여주는 것도 좋을 것 같다고 생각해 키워드 추출 부분을 중간에 추가하였습니다. ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:0:0","tags":["NLP"],"title":"[Project] SNS 감정분석을 통한 여론 조사","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"Noun - Corpus 생성 우선 수집한 트윗 데이터에서, 키워드가 될 수 있는 명사 데이터만 추출하였습니다. 아무래도 텍스트 데이터에는 키워드라고 부르기 어려운 단어들이 많아서, 이부분을 걸러내기 위함입니다. 형태소 분석기 중 kkma에서 텍스트 데이터에서 명사를 추출하는 기능이 있어서 이를 활용하였습니다. import pandas as pd import numpy as np import pickle from konlpy.tag import Kkma pName = [#Keywords] for name in pName: df = pd.read_csv('#Path'+name+'.csv') tw = df['Tweet'] i = 0 twList = [] original = [] for twData in tw: noun = okt.nouns(twData) original.append(twData) string = \"\" for idx, v in enumerate(noun): if len(v) \u003c 2: noun.pop(idx) string = ' '.join(noun) twList.append(string) i += 1 if (i % 1000 == 0): print(\"Running....\") with open('#Path'+name+'.pkl', 'wb') as file: pickle.dump(twList, file) print(\"End: \"+name) with open('#Path'+name+'.pkl', 'wb') as file: pickle.dump(twList, file) ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:1:0","tags":["NLP"],"title":"[Project] SNS 감정분석을 통한 여론 조사","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"키워드 추출 KR-WordRank는 비지도학습을 기반으로 한국어 키워드를 추출합니다. 이를 활용해 생성한 Noun - Corpus에서 상위 7개의 키워드를 추출했습니다. from krwordrank.word import KRWordRank from krwordrank.sentence import summarize_with_sentences stopwords = {} # Stopwords you want to except penalty = lambda x:0 if (25 \u003c= len(x) \u003c= 80) else 1 keywords, sents = summarize_with_sentences( twList, penalty=penalty, # penalty of word-length: remove too short and too long stopwords = stopwords, diversity=0.5, # Diversity of keywords num_keywords=100, num_keysents=10, verbose=False ) pName = [#Keywords] for name in pName: df = pd.read_csv('#Path'+name+'.csv') tw = df['Tweet'] twList = [] nounStr = \"\" for twData in tw: noun = okt.nouns(twData) nounStr = ' '.join(noun) nounPos = kkma.pos(nounStr) string = \"\" onlyNNP = [] for (text, pos) in nounPos: if pos == 'NNP': onlyNNP.append(text) string = ' '.join(onlyNNP) twList.append(string) with open('#Path'+name+'_NNP.pkl', 'wb') as file: pickle.dump(twList, file) 어플리케이션 처리한 데이터를 사용자에게 보여주기 위해 웹 페이지를 구현하기로 하였습니다. 웹페이지를 구성한다면 Django, Flask등을 활용하여 웹 어플리케이션을 만들게 되는데, 저는 Streamlit이라는 파이썬 웹 어플리케이션 툴을 사용하기로 하였습니다. Streamlit은 기계학습과 데이터 과학을 위한 웹 앱을 편리하게 만들수 있도록 합니다. Streamlit을 통해 로컬 환경에서 웹 어플리케이션을 만들어 시연해보았습니다. 후기 작년 하반기에 만들었던 프로젝트를 문서화 하였습니다. 과제나 학습이 아닌 프로젝트를 처음으로 해보게 되었는데, 감회가 새로웠습니다. 프로그램을 설계하고, 구현하는 과정에서 내가 모르는 것들을 찾아가는 것이 좋았습니다. 다만 너무 외부 오픈소스에 의존해서 만들었다는 점이 아쉽게 느껴지고, 만들다보니 개선할 점도 많이 나온 것 같아 좀 아쉽다는 느낌이 듭니다. ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:2:0","tags":["NLP"],"title":"[Project] SNS 감정분석을 통한 여론 조사","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"Colab환경에서 Twitter 크롤링 ","date":"2021-09-07","objectID":"/twitterapi/:0:0","tags":["NLP"],"title":"Colab 환경에서 트윗 데이터 수집하기","uri":"/twitterapi/"},{"categories":["project"],"content":"Twitter API 생성 트위터 가입 개발자 계정 등록하기 API생성 참고) 트위터 API 승인받기 ","date":"2021-09-07","objectID":"/twitterapi/:1:0","tags":["NLP"],"title":"Colab 환경에서 트윗 데이터 수집하기","uri":"/twitterapi/"},{"categories":["project"],"content":"트윗 검색 데이터 수집 + Pororo를 활용한 감정 분석 twitter_python twitter_consumer_key = \"\" twitter_consumer_secret = \"\" twitter_access_token = \"\" twitter_access_secret = \"\" import twitter twitter_api = twitter.Api(consumer_key=twitter_consumer_key, consumer_secret=twitter_consumer_secret, access_token_key=twitter_access_token, access_token_secret=twitter_access_secret) query = \" \" elements = twitter_api.GetSearch(term = query, count = 10000) pos = 0 pos_list = [] neg = 0 neg_list = [] senti_Analysis = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") for status in statuses: #print(senti_Analysis(status.text)) if (senti_Analysis(status.text) == \"Negative\"): neg += 1 neg_list.append(status.text) if (senti_Analysis(status.text) == \"Positive\"): pos += 1 pos_list.append(status.text) tweepy import tweepy from tweepy import Stream from tweepy import OAuthHandler from tweepy.streaming import StreamListener auth = tweepy.OAuthHandler(consumer_key = twitter_consumer_key, consumer_secret = twitter_consumer_secret) api = tweepy.API(auth) from pororo import Pororo query = \"KeyWord\" pos = 0 pos_list = [] neg = 0 neg_list = [] sa = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") res = [] for tw in tweepy.Cursor(api.search, q = query, since = \"2021-07-14\").items(): tw_txt = tw.text if sa(tw_txt) == \"Positive\": pos += 1 pos_list.append(tw_txt) if sa(tw_txt) == \"Negative\": neg += 1 neg_list.append(tw_txt) res.append(tw_txt) print(pos) print(neg) ","date":"2021-09-07","objectID":"/twitterapi/:2:0","tags":["NLP"],"title":"Colab 환경에서 트윗 데이터 수집하기","uri":"/twitterapi/"},{"categories":["project"],"content":"Twitter API의 한계 1주전 트윗까지만 접근 가능 쿼리 속도 제한 : 5분당 180쿼리 ","date":"2021-09-07","objectID":"/twitterapi/:2:1","tags":["NLP"],"title":"Colab 환경에서 트윗 데이터 수집하기","uri":"/twitterapi/"},{"categories":["project"],"content":"twint import twint c = twint.Config() c.Search = '페이커' c.Limit = 5 c.Since = '2021-09-13' c.Until = '2021-09-14' c.Output = \"test.json\" c.Popular_tweets = False c.Store_json = True twint.run.Search(c) 익명으로 사용 가능 거의 모든 트윗 수집 가능 속도 제한 없음 테스트 결과 성능이 매우 좋지 못하였음 참고) 파이썬과 트위터 API를 활용한 트위터 크롤링 트위터 파헤치기 시리즈 첫번째 - 수집하기 ","date":"2021-09-07","objectID":"/twitterapi/:3:0","tags":["NLP"],"title":"Colab 환경에서 트윗 데이터 수집하기","uri":"/twitterapi/"},{"categories":["NLP"],"content":"프로젝트를 위한 NLP 공부","date":"2021-08-24","objectID":"/nlp_05/","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"Splitting Data ","date":"2021-08-24","objectID":"/nlp_05/:0:0","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"Superivised Learning(지도학습) 정답이 있는 데이터를 활용하여 학습 입력값(X_Label)과 결과값(Y_Label)이 있는 테스트 데이터를 학습시킨다. ","date":"2021-08-24","objectID":"/nlp_05/:1:0","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"X_Label과 Y_Label 분리하기 ","date":"2021-08-24","objectID":"/nlp_05/:2:0","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"zip함수 sequence = [['a', 1], ['b', 2], ['c', 3]] x,y = zip(*sequence) #데이터 unpack위해 *사용 print(x) print(y) ('a', 'b', 'c') (1, 2, 3) ","date":"2021-08-24","objectID":"/nlp_05/:2:1","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"DataFrame import pandas as pd values = [['당신에게 드리는 마지막 혜택!', 1], ['내일 뵐 수 있을지 확인 부탁드...', 0], ['도연씨. 잘 지내시죠? 오랜만입...', 0], ['(광고) AI로 주가를 예측할 수 있다!', 1]] columns = ['메일 본문', '스팸 메일 유무'] df = pd.DataFrame(values, columns=columns) x_label = df['메일 본문'] y_label = df['스팸 메일 유무'] print(\"[X_Label]\") print(x_label) print(\"\\n[Y_Label]\") print(y_label) [X_Label] 0 당신에게 드리는 마지막 혜택! 1 내일 뵐 수 있을지 확인 부탁드... 2 도연씨. 잘 지내시죠? 오랜만입... 3 (광고) AI로 주가를 예측할 수 있다! Name: 메일 본문, dtype: object [Y_Label] 0 1 1 0 2 0 3 1 Name: 스팸 메일 유무, dtype: int64 ","date":"2021-08-24","objectID":"/nlp_05/:2:2","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"테스트 데이터 분리 x와 y가 이미 분리된 데이터에 대한 테스트 데이터 분리 과정 ","date":"2021-08-24","objectID":"/nlp_05/:3:0","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"sklearn train_test_split을 활용해 테스트 데이터 분리 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=42) x: 독립 변수 데이터(배열, 데이터 프레임) y: 종속 변수 데이터, 레이블 데이터 test_size: 테스트용 데이터 개수, 1보다 작을 시 비율 의미 train_size: 학습용 데이터의 개수, 1바도 작을 시 비율 의미 random_state: 난수 시드 import numpy as np from sklearn.model_selection import train_test_split x,y = np.arange(10).reshape((5,2)), range(5) #분리된 x,y 데이터 생성 y = list(y) print(x) print(y) print(\"\\n\\n\") x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 1234) # 전체 데이터 중 1/3을 테스트 데이터로 지정 # random_state의 seed_value = 42 print(\"[X_train]\") print(x_train) print(\"[X_test]\") print(x_test) print(\"\\n[Y_train]\") print(y_train) print(\"[Y_test]\") print(y_test) [[0 1] [2 3] [4 5] [6 7] [8 9]] [0, 1, 2, 3, 4] [X_train] [[2 3] [4 5] [6 7]] [X_test] [[8 9] [0 1]] [Y_train] [1, 2, 3] [Y_test] [4, 0] 참고) 딥러닝을 이용한 자연어 처리 입문 ","date":"2021-08-24","objectID":"/nlp_05/:3:1","tags":["NLP"],"title":"[NLP] 5.텍스트 전처리 - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"프로젝트를 위한 NLP 공부","date":"2021-08-18","objectID":"/nlp_04/","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"정수 인코딩 컴퓨터는 텍스트보다 숫자를 더 잘 처리할 수 있다. 이 특징을 살리기 위해 NLP에서는 텍스트를 숫자로 바꾸는 기법들을 제안한다. 이러한 기법들은 각 단어를 고유한 정수값에 Mapping시키는 전처리 작업이 요구되기도 한다. ","date":"2021-08-18","objectID":"/nlp_04/:0:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Dictionary from nltk.tokenize import sent_tokenize from nltk.tokenize import word_tokenize from nltk.corpus import stopwords text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\" text = sent_tokenize(text) #문장 토큰화 vocab = {} sentence = [] stop_words = set(stopwords.words('english')) for i in text: s = word_tokenize(i) # 문장에 대해 단어 토큰화 res = [] for w in s: w = w.lower() #대문자 -\u003e 소문자 if w not in stop_words: #불용어 제거 if len(w) \u003e 2: # 단어 길이가 2이하인 경우 제거 res.append(w) if w not in vocab: vocab[w] = 0 vocab[w] += 1 sentence.append(res) print(\"Word Tokenize\") print(sentence) # 단어토큰화 print(\"Word Frequency\") print(vocab) # 중복을 제거한 단어의 빈도수 vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse= True) #빈도수 순으로 정렬 print(vocab_sorted) word_to_idx = {} #빈도수 높은 순서대로 고유번호 부여 idx = 0 for (word, freq) in vocab_sorted: if freq \u003e 1: idx += 1 word_to_idx[word] = idx print(\"Encoded Word\") print(word_to_idx) Word Tokenize [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']] Word Frequency {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1} [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)] Encoded Word {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7} word_to_idx를 이용하여 토큰화된 sentence의 각 단어를 정수로 변환시킬 수 있다. ex) [‘barber’, ‘person’] -\u003e [1,5] 단어 집합에 존재하지 않는 단어들은 Out-Of-Voca라고 하며, 새로운 index르 인코딩한다. word_to_idx['OOV'] = len(word_to_idx) + 1 ","date":"2021-08-18","objectID":"/nlp_04/:0:1","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Counter from collections import Counter words = sum(sentence, []) #단어집합을 만들기 위해 문장의 경계인 []제거 vocab = Counter(words) #중복을 제거하고 단어의 빈도수 기록 vocab = vocab.most_common(5) #등장 빈도수가 높은 상위 5개의 단어만 저장 print(vocab) [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)] ","date":"2021-08-18","objectID":"/nlp_04/:0:2","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"NLTK - FreqDist Counter()와 사용방법이 동일 from nltk import FreqDist import numpy as np vocab = FreqDist(np.hstack(sentence)) #문장 구분 제거 == sum(sentence, []) print(vocab.most_common(5)) [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)] ","date":"2021-08-18","objectID":"/nlp_04/:0:3","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"enumerate enumerate: 순서가 있는 자료형(list, set, tuple,dictionary, string)을 입력받아 인덱스를 순차적으로 리턴 word_to_index = {word[0]: index+1 for index, word in enumerate(vocab)} print(word_to_idx) {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7} ","date":"2021-08-18","objectID":"/nlp_04/:0:4","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Keras Keras는 기본적인 전처리 도구를 제공 정수 인코딩의 경우, Keras의 Tokenizer를 사용하기도 한다. from tensorflow.keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() tokenizer.fit_on_texts(sentences) #sentences: 단어 토큰화된 데이터 print(\"[Word index]\") # 인덱스 출력 print(tokenizer.word_index) print(\"[Word count]\") # 카운트 출력 print(tokenizer.word_counts) print(\"[sequences]\") # corpus를 인덱스로 변환 print(tokenizer.texts_to_sequences(sentence)) [Word index] {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13} [Word count] OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]) [sequences] [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]] Keras Tokenizer는 기본적으로 OOV에 대해 정수 인코딩 과정에서 단어를 제거한다는 특징을 가진다. 따라서 OOV를 보존하고 싶으면 인자 oov_token을 사용한다. voca_size = 5 # 빈도수 상위 5개 단어만 사용, OOV와 숫자 0 고려하여 크기는 +2 tokenizer = Tokenizer(num_words= voca_size + 2, oov_token='OOV') tokenizer.fit_on_texts(sentences) print(tokenizer.texts_to_sequences(sentences)) [[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]] Padding 기계는 길이가 동일한 문서에 대해 행렬 연산이 가능 병렬 연산을 위해서 여러 문장의 길이를 동일하게 맞춰주는 작업 데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 작업 ","date":"2021-08-18","objectID":"/nlp_04/:1:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Numpy 사용 import numpy as np max_len = max(len(item) for item in encoded) #문장의 최대 길이 구하기 for item in encoded: while len(item) \u003c max_len: item.append(0) # 숫자 0 사용 -\u003e Zero padding padded_np = np.array(encoded) print(padded_np) [[ 1 5 0 0 0 0 0] [ 1 8 5 0 0 0 0] [ 1 3 5 0 0 0 0] [ 9 2 0 0 0 0 0] [ 2 4 3 2 0 0 0] [ 3 2 0 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 2 0 0 0 0] [ 7 7 3 2 10 1 11] [ 1 12 3 13 0 0 0]] ","date":"2021-08-18","objectID":"/nlp_04/:2:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"keras 사용 from tensorflow.keras.preprocessing.sequence import pad_sequences encoded = tokenizer.texts_to_sequences(sentences) pad_seq = pad_sequences(encoded) print(pad_seq) [[ 1 5 0 0 0 0 0] [ 1 8 5 0 0 0 0] [ 1 3 5 0 0 0 0] [ 9 2 0 0 0 0 0] [ 2 4 3 2 0 0 0] [ 3 2 0 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 2 0 0 0 0] [ 7 7 3 2 10 1 11] [ 1 12 3 13 0 0 0]] ","date":"2021-08-18","objectID":"/nlp_04/:3:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"길이 제한, 값 수정 # 길이제한: maxlen = 5 # value 수정: 사용된 정수와 겹치지 않게 단어 집합 크기보다 1 큰 수 사용 padded = pad_sequences(encoded, maxlen=5, padding='post', value = len(tokenizer.word_index)+1) print(padded) # 길이가 maxlen(= 5)보다 작으면 0으로 패딩, 크면 데이터 소멸 [[ 1 5 14 14 14] [ 1 8 5 14 14] [ 1 3 5 14 14] [ 9 2 14 14 14] [ 2 4 3 2 14] [ 3 2 14 14 14] [ 1 4 6 14 14] [ 1 4 6 14 14] [ 1 4 2 14 14] [ 3 2 10 1 11] [ 1 12 3 13 14]] One-Hot Encoding ","date":"2021-08-18","objectID":"/nlp_04/:3:1","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Vocabulary(단어집합) 서로 다른 단어들의 집합 텍스트 데이터의 모든 단어를 중복을 허용하지 않고 모아놓을 것을 지칭 One-Hot encoding은 단어 집합을 만들고 각 단어에 정수 인덱스를 부여하는 것에서부터 시작된다. ","date":"2021-08-18","objectID":"/nlp_04/:4:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"One-Hot Encoding 단어 집합의 크기를 백터의 차원으로 하고, 표현하고자 하는 단어의 인덱스에 1의 값을 부여하는 단어의 벡터 표현 방식. 이렇게 표현된 벡터를 One-Hot vector라고 한다. ","date":"2021-08-18","objectID":"/nlp_04/:5:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Keras 활용 from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import to_categorical text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\" sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\" # 1. 정수 인코딩 t = Tokenizer() t.fit_on_texts([text]) # 각 단어에 대한 인코딩 print(t.word_index) encoded = t.texts_to_sequences([sub_text])[0] #[[1,2,3,4]] -\u003e [1,2,3,4] print(encoded) # 2. One-Hot vector 생성 one_hot = to_categorical(encoded) print(one_hot) {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7} #단어 인덱스 [2, 5, 1, 6, 3, 7] # 정수 인코딩 [[0. 0. 1. 0. 0. 0. 0. 0.] # 원-핫 인코딩 [0. 0. 0. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1.]] ","date":"2021-08-18","objectID":"/nlp_04/:6:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"한계점 단어의 개수가 늘어날 수록, 벡터를 저장하기 위한 공간이 늘어난다. 단어의 유사도를 반영할 수 없음 ex) book과 books를 일반적으로 다른 단어라고 취급 참고) 딥러닝을 이용한 자연어 처리 입문 ","date":"2021-08-18","objectID":"/nlp_04/:7:0","tags":["NLP"],"title":"[NLP] 4.텍스트 전처리 - 인코딩 \u0026 패딩","uri":"/nlp_04/"},{"categories":["NLP"],"content":"프로젝트를 위한 NLP 공부","date":"2021-08-17","objectID":"/nlp_03/","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Cleaning and Normalization Tokenization 전, 텍스트 데이터를 용도에 맞게 정제(cleaning)및 정규화(normalization)하는 일 정제(cleaning): 가지고 있는 corpus로부터 noise data제거 정규화(normalization): 표현 방법이 다른 단어들을 통합, 같은 단어로 재구성 ","date":"2021-08-17","objectID":"/nlp_03/:0:0","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"규칙에 기반한 표기가 다른 단어 통합 같은 의미를 갖고 있는 표기가 다른 단어들을 하나의 단어로 정규화 ","date":"2021-08-17","objectID":"/nlp_03/:1:0","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Lemmatization(표제어 추출) 단어들로부터 표제어를 찾아가는 과정 ex) am, are, is -\u003e be 형태학적 파싱 단어의 어간(stem, 단어의 의미를 가지는 핵심 부분)과 접사(affix, 단어의 추가적인 의미 부여)를 분리하는 작업 NLTK에서는 WorkNetLemmatizer를 지원 from nltk.stem import WordNetLemmatizer n=WordNetLemmatizer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([n.lemmatize(w) for w in words]) ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting'] # 'dy', 'ha': 적절하지 못한 단어 # lemmatizer는 단어의 품사 정보를 알아야만 정확한 결과를 도출한다. # WordNetLemmatizer는 입력으로 단어의 품사 정보 제공 가능 ex) n.lemmatize('has', 'v') -\u003e 단어의 형태가 어느정도 보존되는 특징을 가진다. ","date":"2021-08-17","objectID":"/nlp_03/:1:1","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Stemming(어간 추출) 단어에서 어간(stem)을 추출하는 과정 섬세한 작업이 아니기 때문에, 어간추출후에 나오는 결과는 사전에 없는 단어인 경우도 있다. 일반적으로 표제어 추출보다 빠른 특성을 가진다. NLTK에서는 Porter stemmer와 Lancaster Stemmer를 지원한다. #Porter Stemmer from nltk.stem import PorterStemmer s=PorterStemmer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([s.stem(w) for w in words]) ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start'] from nltk.stem import LancasterStemmer l=LancasterStemmer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([l.stem(w) for w in words]) ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start'] -\u003e Stemmer 알고리즘에 따라 동일한 단어에서도 다른 결과를 출력하기 때문에, 어떤 Stemmer가 적합한지 판단하고 사용해야한다. ","date":"2021-08-17","objectID":"/nlp_03/:1:2","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"대, 소문자 통합 영어권에서 대문자의 경우, 특정 상황에서만 쓰이기 때문에, 대문자를 소문자로 반환하는 형식으로 단어의 수를 줄일 수 있다. ","date":"2021-08-17","objectID":"/nlp_03/:2:0","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"불필요한 단어 제거 Noise data: 아무 의미를 가지지 않는 글자(특수문자) + 분석하고자 하는 목적에 맞지 않는 불필요한 단어 ","date":"2021-08-17","objectID":"/nlp_03/:3:0","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"불용어 제거 불용어(Stopword): 문장의 실제 의미 분석을 하는데 도움되지 않는 단어(I, my, me, over …) 직접 불용어를 정의해서 사용하거나, 패키지에서 정의된 불용어를 사용 NLTK에서는 100여개 이상의 단어를 불용어로 패키지 내에서 정의하고 있다. from nltk.corpus import stopwords from nltk.tokenize import word_tokenize example = \"Family is not an important thing. It's everything.\" stop_words = set(stopwords.words('english')) word_tokens = word_tokenize(example) result = [] for w in word_tokens: if w not in stop_words: # 단어가 불용어가 아니면 추가 result.append(w) ","date":"2021-08-17","objectID":"/nlp_03/:3:1","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"등장 빈도가 적은 단어 제거 텍스트 데이터에서 너무 적게 등장하여 자연어 처리에 도움이 되지 않는 데이터들을 제거. ","date":"2021-08-17","objectID":"/nlp_03/:3:2","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"길이가 짧은 단어 ex) 100000개의 메일 데이터에서 3~4번 밖에 등장하지 않는 데이터 제거 영어권 언어의 경우 길이가 짧은 언어들을 제거하는 것은 문장에서 크게 의미가 없는 단어들을 제거하는 효과를 보여준다. 길이가 2~3 이하인 단어들을 제거하는 것은 많은 수의 불용어(it, at, on …)들을 제거하는데 효과적이나, 길이가 짧은 명사들(ox, fox, cat …) 또한 제거될 가능성이 있다. ","date":"2021-08-17","objectID":"/nlp_03/:3:3","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Regular Expression 특정한 규칙을 가진 문자열의 집합을 표현하는데 사용하는 형식 언어. 문자열을 처리하는 방법 중의 하나로, 정규 표현식을 사용하면 특정한 조건의 문자를 ‘검색’하거나 ‘치환’하는 과정을 매우 간편하게 처리할 수 있다. 정규표현식을 이용하면 코퍼스 내부에서 반복적으로 등장하는 글자들을 규칙에 기반하여 쉽게 제거할 수 있다. # 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제 import re text = \"I was wondering if anyone out there could enlighten me on this car.\" shortword = re.compile(r'\\W*\\b\\w{1,2}\\b') print(shortword.sub('', text)) was wondering anyone out there could enlighten this car. 참고) 딥러닝을 이용한 자연어 처리 입문 ","date":"2021-08-17","objectID":"/nlp_03/:4:0","tags":["NLP"],"title":"[NLP] 3.텍스트 전처리 - 정제및 정규화","uri":"/nlp_03/"},{"categories":["NLP"],"content":"프로젝트를 위한 NLP 공부","date":"2021-08-15","objectID":"/nlp_02/","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Text Preprocessing Tokenization 주어진 corpus에서 토큰(token)이라 불리는 단위로 나누는 작업을 지칭한다 ","date":"2021-08-15","objectID":"/nlp_02/:0:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Word Tokenization 토큰의 기준을 단어(word)로 하는 경우, 단어 토른화라고 한다. ex) 구두점(. , ? ; !)을 제외, 띄어쓰기를 기준으로 토큰화 Time is an illusion. Lunchtime double so! \u003e \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\" ","date":"2021-08-15","objectID":"/nlp_02/:1:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"고려 사항 구두점, 특수 문자 제외 여부 선택 ex) 2021/08/15: ‘/‘가 날짜를 구분해 주는 역할 줄임말, 단어 내 띄어쓰기 ex) New York: “New\"와 “York\"로 분리시 본래 의미가 없어질 수 있음 ","date":"2021-08-15","objectID":"/nlp_02/:1:1","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Penn Treebank Tokenization 표준으로 쓰이는 토큰화 방법 중 하나 Rule) 하이픈(’-’)으로 구성된 단어는 하나로 유지 아포스트로피(’`’)로 “접어\"가 함께하는 단어는 분리 from nltk.tokenize import TreebankWordTokenizer tokenizer = TreebankWordTokenizer() txt = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\" print(tokenizer.tokenize(txt)) ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] ","date":"2021-08-15","objectID":"/nlp_02/:1:2","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Sentence Tokenization 토큰의 단위가 문장(sentence)인 경우 corpus를 문장 단위로 분류하기 위해서는, corpus가 어떤 국적의 언어인지, 해당 corpus에서 특수문자들이 어떻게 사용되고 있는지에 따라 직접 규칙을 정의할 필요가 있다. # NLTK에서 지원하는 영어문장 토큰화 - sent_tokenize import nltk nltk.download('punkt') from nltk.tokenize import sent_tokenize sent_tokenize(text) text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\" ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.'] # 한국어 문장 토큰화 도구 import kss txt = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?' print(kss.split_sentences(txt)) ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?'] ","date":"2021-08-15","objectID":"/nlp_02/:2:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Binary Classifier 문장 토큰화의 예외 사항을 발생시키는 마침표의 처리를 위해, 입력에 따라 두 개의 클래스로 분류하는 이진 분류기(binary classifier)를 사용하기도 한다. 두 개의 클래스 마침표(.)가 단어의 일부분(약어)인 경우 마침표(.)가 문장의 구분자인 경우 ","date":"2021-08-15","objectID":"/nlp_02/:3:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"한국어 토큰화의 어려움 교착어 어근과 접사에 의해 단어의 기능이 결정. 형태소(morpheme)의 개념 이해가 필수적: 형태소 토큰화 -\u003e 형태소: 뜻을 가진 가장 작은 말의 단위 자립 형태소: 접사, 어미, 조사와 상관없이 사용할 수 있는 형태소 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소 띄어쓰기가 지켜지지 않는 corpus가 비교적 많음 ","date":"2021-08-15","objectID":"/nlp_02/:4:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"품사 태깅(Part-of-speech tagging) 단어의 표기는 같으나, 품사에 따라서 단어의 의미가 달라지기도 함. 단어의 의미를 제대로 파악하기 위해서는 단어가 어떤 품사로 쓰였는지 구분하는 것이 주요 지표가 될 수 있음. ","date":"2021-08-15","objectID":"/nlp_02/:5:0","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"nltk pos_tag from nltk.tokenize import word_tokenize from nltk.tag import pos_tag text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\" a = word_tokenize(text) pos_tag(a) [('I', 'PRP'), #인칭대명사 ('am', 'VBP'), #동사 ('actively', 'RB'), #부사 ('looking', 'VBG'), #현재부사 ('for', 'IN'), #전치사 ('Ph.D.', 'NNP'), #고유명사 ('students', 'NNS'), #복수형명사 ('.', '.'), ('and', 'CC'), #접속사 ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), #관사 ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')] ","date":"2021-08-15","objectID":"/nlp_02/:5:1","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"KoNLPy 한국어 자연어 처리를 위한 파이선 패키지 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma) 지원 형태소 분석기 마다 성능과 결과가 다르게 나오기 때문에, 필요 용도에 따라 어던 형태소 분석기가 적절한지 판단하고 사용. #okt from konlpy.tag import Okt okt=Okt() print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요'] [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')] ['코딩', '당신', '연휴', '여행'] #kkma from konlpy.tag import Kkma kkma=Kkma() print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\")) ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요'] [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')] ['코딩', '당신', '연휴', '여행'] 참고) 딥러닝을 이용한 자연어 처리 입문 ","date":"2021-08-15","objectID":"/nlp_02/:5:2","tags":["NLP"],"title":"[NLP] 2.텍스트 전처리 - 토큰화","uri":"/nlp_02/"},{"categories":["NLP"],"content":"프로젝트를 위한 NLP 공부","date":"2021-08-15","objectID":"/nlp_01/","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"NLP기초 실제 NLP 관련 프로젝트에 들어가기 앞서, NLP 기초를 다지고자 한다 ","date":"2021-08-15","objectID":"/nlp_01/:0:0","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"머신러닝 프레임워크와 라이브러리 ","date":"2021-08-15","objectID":"/nlp_01/:1:0","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Tensorflow 머신러닝 오픈소스 라이브러리 머신러닝과 딥러닝을 직관적이고 손쉽게 할 수 있도록 설계 ","date":"2021-08-15","objectID":"/nlp_01/:1:1","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Keras 텐서플로우에 대한 추상화된 API 제공 백엔드로 텐서플로우 사용, 좀 더 쉽게 딥 러닝을 할 수 있도록 도움 ","date":"2021-08-15","objectID":"/nlp_01/:1:2","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Gensim 머신러닝을 사용하여 트픽 모델링과 자연어 처리등을 수행할 수 있게 해주는 오픈소스 라이브러리 ","date":"2021-08-15","objectID":"/nlp_01/:1:3","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Scikit-learn 파이썬 머신러닝 라이브러리 Naive bayse, SVM등 다양한 머신 러닝 모듈을 불러올 수 있음 자체 데이터 제공(아이리스 데이터, 당뇨병 데이터 등등..) ","date":"2021-08-15","objectID":"/nlp_01/:1:4","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"NLTK 자연어 처리를 위한 파이썬 패키지 ","date":"2021-08-15","objectID":"/nlp_01/:1:5","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"KoNLPy 한국어 자연어 처리를 위한 형태소 분석기 패키지 ","date":"2021-08-15","objectID":"/nlp_01/:1:6","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"데이터 분석 패키지 ","date":"2021-08-15","objectID":"/nlp_01/:2:0","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Pandas 파이썬 데이터 처리를 위한 라이브러리 세 종류의 데이터 구조 사용 Series 1차원 배열의 값(value)에 각 값에 대응되는 인덱스(index)를 부여할 수 있는 구조 import pandas as pd sr = pd.Series([100,200,300,400], index = [\"a\",\"b\",\"c\",\"d\"]) print(sr) a 100 b 200 c 300 d 400 dtype: int64 DataFrame 2차원 리스트를 매개변수로 전달. 행 방향 인덱스와 열 방향 인덱스가 존재 -\u003e 행과 열을 가지는 자료구조 List, Series, Dict, ndarrays등을 통해 생성할 수 있다. values = [[1,2,3], [4,5,6], [7,8,9]] index = [\"I1\",\"I2\",\"I3\"] columns = [\"C1\",\"C2\",\"C3\"] df = pd.DataFrame(values, index=index, columns=columns) print(df) C1 C2 C3 I1 1 2 3 I2 4 5 6 I3 7 8 9 # 리스트로 생성하기 data = [ ['1000', 'Steve', 90.72], ['1001', 'James', 78.09], ['1002', 'Doyeon', 98.43], ['1003', 'Jane', 64.19], ['1004', 'Pilwoong', 81.30], ['1005', 'Tony', 99.14], ] df = pd.DataFrame(data) print(df) 0 1 2 0 1000 Steve 90.72 1 1001 James 78.09 2 1002 Doyeon 98.43 3 1003 Jane 64.19 4 1004 Pilwoong 81.30 5 1005 Tony 99.14 data = { '학번' : ['1000', '1001', '1002', '1003', '1004', '1005'], '이름' : [ 'Steve', 'James', 'Doyeon', 'Jane', 'Pilwoong', 'Tony'], '점수': [90.72, 78.09, 98.43, 64.19, 81.30, 99.14]} df = pd.DataFrame(data) print(df) 학번 이름 점수 0 1000 Steve 90.72 1 1001 James 78.09 2 1002 Doyeon 98.43 3 1003 Jane 64.19 4 1004 Pilwoong 81.30 5 1005 Tony 99.14 Panel ","date":"2021-08-15","objectID":"/nlp_01/:2:1","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Numpy 수치 데이터를 다루는 파이썬 패키지 행렬 자료구조인 ndarray를 통해 선형 대수 계산에 많이 사용 np.array() 리스트, 튜플, 배열로부터 ndarray생성 인덱스가 항상 0으로 시작함 ndarr1 = np.array([1,2,3],[4,5,6]) print(ndarr1.shape()) (2,3) # 2 x 3 행렬 ndarray초기화 ndarr2 = np.zeros(2,3) # 모든 값이 0인 2x3 행렬 ndarr3 = np.full((2,2), 4) # 모든 값이 특정 상수값인 행렬 ndarr4 = np.eye(3) # 대각선 1, 나머지는 0인 3x3 행렬 print(ndarr2) print(ndarr3) [[0,0,0] [0,0,0]] [[4,4] [4,4]] [[1,0,0] [0,1,0] [0,0,1]] np.arange() 지정해준 범위에 대해서 배열 생성 numpy.arange(start, stop, stem, dtype) a = np.arange(1,10,2) # 1부터 9까지 +2 [1,3,5,7,9] reshape() 배열을 다차원으로 변형 a = np.array(np.arange(30)).reshape((5,6)) print(a) [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]] ","date":"2021-08-15","objectID":"/nlp_01/:2:2","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Matplotlib 데이터를 차트나 플롯으로 시각화하는 패키지 ","date":"2021-08-15","objectID":"/nlp_01/:2:3","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Machine Learning Workflow 출처: 딥러닝을 이용한 자연어 처리 입문출처: 딥러닝을 이용한 자연어 처리 입문 \" 출처: 딥러닝을 이용한 자연어 처리 입문 (출처: 딥러닝을 이용한 자연어 처리 입문) ","date":"2021-08-15","objectID":"/nlp_01/:3:0","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"수집(Acquisition) 머신 러닝을 위한 데이터 수집 자연어 데이터: Corpus(수집된 텍스트의 집합) ","date":"2021-08-15","objectID":"/nlp_01/:3:1","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"점검 및 탐색(Inspection and exploration) 수집한 데이터를 점검, 탐색 머신러닝 적용을 위해서 어떻게 데이터를 정제할지 파악 탐색적 데이터 분석(Exploratory Data Analysis, EDA)단계라고도 부름 ","date":"2021-08-15","objectID":"/nlp_01/:3:2","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"전처리 및 정제(Preprocessing and Cleaning) 토큰화, 정제, 정규화, 불용어 제거 단계 ","date":"2021-08-15","objectID":"/nlp_01/:3:3","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"모델링 및 훈련(Modeling and Training) 머신러닝에 대한 코드를 작성하는 단계 전처리가 완료된 데이터를 머신러닝 알고리즘을 통해 기계에 학습 대부분의 경우, 모든 데이터를 학습시키지 않고, 훈련용 데이터만 학습시켜 과적합(Overfitting)상황을 피한다. 출처: 딥러닝을 이용한 자연어 처리 입문출처: 딥러닝을 이용한 자연어 처리 입문 \" 출처: 딥러닝을 이용한 자연어 처리 입문 (출처: 딥러닝을 이용한 자연어 처리 입문) ","date":"2021-08-15","objectID":"/nlp_01/:3:4","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"평가(Evaluation) 테스트용 데이터로 성능을 평가 기계가 예측한 데이터가 테스트용 데이터의 실제 정답과 얼마나 가까운지 측정 ","date":"2021-08-15","objectID":"/nlp_01/:3:5","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"},{"categories":["NLP"],"content":"배포(Deployment) 완성된 모델을 배포 참고) 딥러닝을 이용한 자연어 처리 입문 ","date":"2021-08-15","objectID":"/nlp_01/:3:6","tags":["NLP"],"title":"[NLP] 1.프레임워크와 라이브러리","uri":"/nlp_01/"}]