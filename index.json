[{"categories":["troubleshooting"],"content":"Springboot í”„ë¡œì íŠ¸ ì¤‘ ë°œìƒí•œ Trouble í•´ê²° ê³¼ì •","date":"2022-04-19","objectID":"/troubleshooting_springjpa/","tags":["springboot"],"title":"[Trouble Shooting] JPAì—ì„œ Like, Containsêµ¬ë¬¸ ì‚¬ìš©ì‹œ ì—ëŸ¬","uri":"/troubleshooting_springjpa/"},{"categories":["troubleshooting"],"content":"React + Springboot + Mysqlì„ ì‚¬ìš©í•œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë˜ ì¤‘ ë°œìƒí•œ ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ë¬¸ì œìƒí™© React.useEffect()ë¥¼ ì‚¬ìš©í•´ ì›¹ í˜ì´ì§€ì— ì ‘ì†ê³¼ ë™ì‹œì—, api ì„œë²„ ìš”ì²­ì„ í†µí•´ íŠ¹ì • ë¬¸ìì—´ì´ í¬í•¨ëœ ë°ì´í„°ë¥¼ DBì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤. JpaRepositoryë¥¼ ì‚¬ìš©í•˜ì—¬ Mysql DBì„œë²„ì— íŠ¹ì • ë¬¸ìì—´ì´ í¬í•¨ëœ DBë¥¼ ì‚¬ìš©í•˜ê³ ì ë‹¤ìŒê³¼ ê°™ì€ Service ì½”ë“œë¥¼ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. // Repository package com.example.modoosugang_be.Repository; import com.example.modoosugang_be.Domain.Lecture; import org.springframework.data.jpa.repository.JpaRepository; import org.springframework.data.jpa.repository.Query; import org.springframework.data.repository.query.Param; import java.util.List; public interface LectureRepository extends JpaRepository\u003cLecture, Long\u003e { List\u003cLecture\u003e findAllByProfessorContains(String professor); } // Service package com.example.modoosugang_be.Service; import com.example.modoosugang_be.Domain.Lecture; import com.example.modoosugang_be.Repository.LectureRepository; import lombok.RequiredArgsConstructor; import org.springframework.stereotype.Service; import java.util.List; @Service @RequiredArgsConstructor public class LectureService { private final LectureRepository lectureRepository; public List\u003cLecture\u003ecallUnivLecture(String univ) { List\u003cLecture\u003e lectures = lectureRepository.findAllByProfessorContains(univ); return lectures; } } ì›ë˜ëŒ€ë¡œë¼ë©´ Jpaë¥¼ í†µí•´ ëª…ëª…ëœ Methodë“¤ì´ ì•Œì•„ì„œ ì˜ ì‘ë™ë˜ì–´ì•¼ í•˜ëŠ”ë°, ì›¬ê±¸? ì´ìƒí•œ ë¬¸ì œìƒí™©ì— ì§ë©´í–ˆë‹¤. ì°¾ì•„ë³´ë‹ˆ JPAì‚¬ìš© ì¤‘ java.lang.IllegalArgumentException ì€ Entity ì‘ì„±ì„ ì˜ëª»í•˜ê±°ë‚˜ í•˜ë©´ ì¢…ì¢… ë°œìƒí•˜ëŠ” ì—ëŸ¬ë¼ê³  í•œë‹¤. ê·¸ëŸ°ë° ë‚´ ê²½ìš°ì—ëŠ” íŠ¹ì´í•˜ê²Œë„ ì„œë²„ë¥¼ ëŒë¦¬ëŠ” ì²« ì‹œí–‰ì—ëŠ” ëŒì•„ê°€ë‹¤ê°€, ë°˜ë³µ ì‹œí–‰ì‹œ ì—ëŸ¬ê°€ ë‚˜ëŠ” ê¸°ì´í•œ(?) í˜„ìƒì´ ë°œìƒí•˜ì˜€ë‹¤. ì›ì¸ë¶„ì„ + í•´ê²° ì—ëŸ¬ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ ì‹œë„ë¥¼ í•´ë´¤ì—ˆë‹¤. Entity \u0026 DB ë§¤ì¹­ ì¬í™•ì¸ Jpaì—ì„œ containsë§ê³  startWith, like ì‚¬ìš©í•´ë³´ê¸° DBì— Columnì„ ì¶”ê°€í•´ì„œ containsë§ê³  findAllByë§Œ ì‚¬ìš©í•˜ê¸° ì‹œë„í•´ë³´ë‹ˆ 3ë²ˆ ë°©ë²•ë§ê³ ëŠ” í•´ê²°ì´ë˜ì§€ ì•Šì•˜ì—ˆë‹¤â€¦ ì§„ì§œë¡œ DBë¥¼ ì—ì–´ì•¼ë˜ë‚˜ ìƒê°í•˜ë˜ ì¤‘, ì´ëŸ° ê±¸ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ë‚´ ê²½ìš°ì™€ëŠ” ë˜‘ê°™ì€ ê²ƒì€ ì•„ë‹ˆì—ˆì§€ë§Œ ìœ„ Issueë¥¼ ë³´ë‹ˆ, Springboot ìµœì‹ ë²„ì „ê³¼ Hibernateë¥¼ ê°™ì´ ì‚¬ìš©ì‹œ startingWith, contains, startsWith, Likeì™€ ê°™ì€ êµ¬ë¬¸ì„ ì‚¬ìš©ì‹œ ë™ì¼í•œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¨ë‹¤ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆì—ˆë‹¤. Participantsë“¤ì˜ ì½”ë©˜íŠ¸ë“¤ì„ ë³´ë‹ˆ, Hibernateë²„ì „ì„ ë‹¤ìš´ê·¸ë ˆì´ë“œ í•˜ê±°ë‚˜, @Queryë¥¼ ì´ìš©í•´ ì§ì ‘ ì¿¼ë¦¬ë¬¸ì„ ì‘ì„±í•˜ë©´ í•´ê²°ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. íŒ€ ë‹¨ìœ„ë¡œ êµ´ëŸ¬ê°€ëŠ” í”„ë¡œì íŠ¸ë‹¤ë³´ë‹ˆ, ë²„ì „ì„ ìˆ˜ì •í•˜ëŠ” ê²ƒë³´ë‹¤ ì§ì ‘ ì¿¼ë¦¬ë¬¸ì„ ì‘ì„±í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí–ˆë‹¤. // ~ì¤‘ëµ~ public interface LectureRepository extends JpaRepository\u003cLecture, Long\u003e { @Query(value = \"SELECT v FROM Lecture v WHERE v.professor Like :univ%\") List\u003cLecture\u003e findLecture(@Param(\"univ\")String univ); } í‰ì†Œì— ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ êµ¬ê¸€ì´ë‚˜ Stackoverflowë§Œ ê²€ìƒ‰í–ˆì—ˆëŠ”ë°, Githubë„ ìœ ì‹¬íˆ ë´ì•¼ê² ë‹¤. ","date":"2022-04-19","objectID":"/troubleshooting_springjpa/:0:0","tags":["springboot"],"title":"[Trouble Shooting] JPAì—ì„œ Like, Containsêµ¬ë¬¸ ì‚¬ìš©ì‹œ ì—ëŸ¬","uri":"/troubleshooting_springjpa/"},{"categories":["project"],"content":"k8s ì „ë¬¸ê°€ ì–‘ì„± ê³¼ì • ì¤‘ ì§„í–‰í•˜ê²Œëœ í”„ë¡œì íŠ¸","date":"2022-04-17","objectID":"/frontend/","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":" Goorm kubernetes ì „ë¬¸ê°€ ì–‘ì„± ê³¼ì • ì¤‘ í”„ë¡œì íŠ¸ ì§„í–‰ ê³¼ì •ì„ ê³µìœ í•©ë‹ˆë‹¤. Front-end: UI êµ¬í˜„ React jsë¥¼ í†µí•´ UIë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. íŒ€ì› ëª¨ë‘ê°€ Reactì‚¬ìš© ê²½í—˜ì´ ì—†ì–´ ê°œì¸ê³µë¶€ë¥¼ ì–´ëŠì •ë„ë¡œ ì‹¤ì‹œí•œ ë‹¤ìŒì— êµ¬í˜„ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì €ëŠ” ìƒí™œì½”ë”©ìœ¼ë¡œ ê¸°ì´ˆì ì¸ ì§€ì‹ì„ ìŒ“ì•˜ìŠµë‹ˆë‹¤. ê°•ì˜í•˜ë‚˜ë§Œìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ì •ë„ê¹Œì§€ ì‹¤ë ¥ì„ ìŒ“ì•˜ìœ¼ë©´ ì¢‹ì•˜ê² ì§€ë§Œ, ì•„ë¬´ë¦¬ UI ë””ìì¸ì„ í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•ŠëŠ”ë‹¤ê³  í•´ë„ ëšë”± ë§Œë“¤ê¸°ì—ëŠ” ì–´ë ¤ì›€ì´ ë§ì•˜ìŠµë‹ˆë‹¤. ê·¸ë˜ë„ êµ¬ê¸€ë§ ì‹ ê³µ(?)ìœ¼ë¡œ ë¦¬ì—‘íŠ¸ í…œí”Œë¦¿ì´ë‚˜ ë‹¤ë¥¸ ì‚¬ëŒì´ ë§ë“  ì½”ë“œë“¤ì„ ë³´ë©° í•˜ë‚˜ë‘˜ì”© UI ì»´í¬ë„ŒíŠ¸ë“¤ì„ ë§Œë“¤ë‹¤ë³´ë‹ˆ, ì–´ë–»ê²Œ ë§Œë“¤ì–´ì•¼í• ì§€ ê·¸ë˜ë„ ê°ì´ ì¡í˜”ìŠµë‹ˆë‹¤. ","date":"2022-04-17","objectID":"/frontend/:0:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"mui muië¼ëŠ” React UI íˆ´ì„ ë§ì´ ì‚¬ìš©í–ˆëŠ”ë°, ì´ê±¸ ì‚¬ìš©í•˜ë‹ˆ ê·¸ë˜ë„ ë´ì¤„ë§Œí•œ UIê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ","date":"2022-04-17","objectID":"/frontend/:1:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"react-table react-tableì€ Reactë¡œ Table UIë¥¼ ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, í…Œì´ë¸” ë‚´ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê±°ë‚˜ ì •ë ¬í•˜ëŠ” ê²ƒì„ ì§€ì›í•´ì£¼ê¸° ë•Œë¬¸ì— ì¢€ ë” ì‰½ê²Œ UIë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. íšŒê³  ì²˜ìŒ ì‹œì‘í•  ë•Œ í”„ë¡ íŠ¸ì—”ë“œëŠ” ëª»í• ê±° ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆëŠ”ë°, ê·¸ë˜ë„ ë§‰ìƒ ë§Œë“¤ë‹¤ë³´ë‹ˆ ì–´ëŠì •ë„ ê°ì´ ì¡íˆê³ , í•´ë³¼ë§Œí•˜ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. Backendìª½ì„ í¬ë§í•´ì„œ í”„ë¡ íŠ¸ì—”ë“œëŠ” ë“±í•œì‹œ í–ˆì—ˆëŠ”ë°, ì„œë¡œ ê¸´ë°€í•˜ê²Œ ì—°ê²°ë˜ëŠ” ë¶„ì•¼ë¼ ë‚˜ì¤‘ì— BEë¥¼ ê³µë¶€í•  ë•Œë„ ë§ì€ ë„ì›€ì´ ë ê±° ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤.(ë¬¼ë¡  backendë¥¼ í•´ë³¸ê±´ ì•„ë‹ˆì§€ë§Œâ€¦ ğŸ˜­) UI êµ¬í˜„í•˜ë©´ì„œ Githubë¡œ í˜‘ì—…í•˜ëŠ” ê²ƒë„ ì²˜ìŒ í•´ë³´ì•˜ëŠ”ë° Organization Repoì—ì„œ forkí•´ì™€ì„œ ì‘ì—…í•˜ê³ , Github branchì „ëµì— ë”°ë¼ ê¸°ëŠ¥ë³„ë¡œ branchë…¸ ë‚˜ëˆ„ì–´ì„œ êµ¬í˜„ í•´ë³´ê³ , Issue\u0026PRë„ ìƒì„±í•´ë³´ë©´ì„œ add, commit, pushë°–ì— ëª¨ë¥´ë˜ gitì„ ì¢€ ë” ì˜ ì“¸ìˆ˜ ìˆê²Œ ëœê±° ê°™ìŠµë‹ˆë‹¤ ã…ã…. ","date":"2022-04-17","objectID":"/frontend/:2:0","tags":["k8s","AWS","react","springboot"],"title":"[k8s project] Frontend UI","uri":"/frontend/"},{"categories":["project"],"content":"k8s ì „ë¬¸ê°€ ì–‘ì„± ê³¼ì • ì¤‘ ì§„í–‰í•˜ê²Œëœ í”„ë¡œì íŠ¸","date":"2022-04-03","objectID":"/design/","tags":["k8s","AWS"],"title":"[k8s project] ì£¼ì œ ì •í•˜ê¸° ~ ì„¤ê³„","uri":"/design/"},{"categories":["project"],"content":" Goorm kubernetes ì „ë¬¸ê°€ ì–‘ì„± ê³¼ì • ì¤‘ í”„ë¡œì íŠ¸ ì§„í–‰ ê³¼ì •ì„ ê³µìœ í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì‹œì‘ êµìœ¡ê³¼ì •ì´ ì „ë¶€ ëë‚˜ê³ , íŒ€í”„ë¡œì íŠ¸ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì–´ì©Œë‹¤ë³´ë‹ˆ íŒ€ì¥ì´ ë˜ì–´ë²„ë ¸ëŠ”ë° ì´ì™• í•˜ëŠ”ê±° ì—´ì‹¬íˆ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ğŸ”¥ğŸ”¥ğŸ”¥ ì£¼ì œ ì •í•˜ê¸° ì•„ë¬´ë˜ë„ êµìœ¡ê³¼ì • ë™ì•ˆ ë°°ì› ë˜ ë‚´ìš©ì´ k8sê°€ ì£¼ê°€ ë˜ì—ˆìœ¼ë‹ˆ, ì´ê²ƒì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í• ë§Œí•œ ì£¼ì œê°€ ë¬´ì—‡ì´ ìˆëŠ”ì§€ íŒ€ì›ë“¤ê³¼ ì˜ë…¼í•´ë³´ì•˜ìŠµë‹ˆë‹¤. k8së¥¼ í™œìš©í•œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì˜ í”„ë¡œì íŠ¸ë“¤ì„ ì°¾ì•„ë³¸ ê²°ê³¼, k8sì˜ HPAë¥¼ ì´ìš©í•˜ì—¬ ì„œë²„ ë¶€í•˜ì— ë”°ë¼ ì ì ˆí•˜ê²Œ Scale in/outí•˜ëŠ” í”„ë¡œì íŠ¸ë“¤ì„ ë³¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. (ì°¸ê³ ) ì €ì˜ íŒ€ë„ Auto Scalingì„ í†µí•´ ì„œë²„ ë¶€í•˜ë¥¼ ì›í™œí•˜ê²Œ ëŒ€ì‘í•˜ëŠ” ê²ƒì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” í”„ë¡œì íŠ¸ë¥¼ í•´ë³´ë©´ ì–´ë–¨ê¹Œë¼ëŠ” ì´ì•¼ê¸°ê°€ ì˜¤ê°”ê³ , íŠ¹ì • ê¸°ê°„ì—ë§Œ ì„œë²„ ë¶€í•˜ê°€ ì§‘ì¤‘ë˜ëŠ” ì„œë¹„ìŠ¤ê°€ ì–´ë–¤ê²Œ ìˆì„ê¹Œ ê³ ë¯¼í•´ë³¸ ê²°ê³¼ ìˆ˜ê°•ì‹ ì²­ ì‚¬ì´íŠ¸ë¥¼ k8së¥¼ í†µí•´ êµ¬í˜„í•´ë³´ë©´ ì¢‹ê² ë‹¤ëŠ” ìƒê°ì„ í–ˆìŠµë‹ˆë‹¤. ì•„ì´ë””ì–´ êµ¬ì²´í™”(ì´ˆì•ˆ) ê¸°ìˆ ìŠ¤íƒ êµìœ¡ê³¼ì •ë™ì•ˆ ë°°ìš´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ + $\\alpha$ í•˜ì—¬ ì‚¬ìš©í•  ê¸°ìˆ ì„ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ì„¤ê³„ (ì•„í‚¤í…ì³ \u0026 UI) 4ëª…ì˜ íŒ€ì›ë“¤ì´ 2ëª…ì”© ë‚˜ëˆ„ì–´ì„œ ì•„í‚¤í…ì³ ì„¤ê³„ì™€ UIì„¤ê³„ë¥¼ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì €ëŠ” ì•„í‚¤í…ì³ ì„¤ê³„ë¥¼ ë‹´ë‹¹í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ","date":"2022-04-03","objectID":"/design/:0:0","tags":["k8s","AWS"],"title":"[k8s project] ì£¼ì œ ì •í•˜ê¸° ~ ì„¤ê³„","uri":"/design/"},{"categories":["project"],"content":"ì•„í‚¤í…ì³ ì„¤ê³„ DevOps ê³¼ì •ì„ ì´ìˆ˜í–ˆë‹¤ë³´ë‹ˆ ì„¤ê³„ì— ìˆì–´ì„œ AWS í´ë¼ìš°ë“œ í™˜ê²½ê³¼ CI/CD ì‹ ê²½ì„ ë§ì´ ì¼ìŠµë‹ˆë‹¤. eksë¥¼ ì‚¬ìš©í•˜ì—¬ k8sí´ëŸ¬ìŠ¤í„° í™˜ê²½ì„ AWSí™˜ê²½ì—ì„œ êµ¬í˜„í•˜ê³ , ì—¬ê¸°ì— ëª¨ë‹ˆí„°ë§/ë¡œê¹… ì‹œìŠ¤í…œì„ ë§ë¶™ì´ê³ , Jenkins/ArgoCDë¡œ ë¹Œë“œ, ë°°í¬í™˜ê²½ì„ ì¡°ì„±í•˜ê¸°ë¡œ ê³„íší•˜ì˜€ìŠµë‹ˆë‹¤. ì„¤ê³„ë¥¼ í•´ë³´ëŠ” ê²ƒì´ ì´ë²ˆì´ ì²˜ìŒì´ë¼ ë¨¸ë¦¬ê°€ ë¹ ê°œì§ˆê²ƒê°™ì•˜ì§€ë§Œ, ì„¤ê³„ë¥¼ í•˜ê³ ë‚˜ë‹ˆ í”„ë¡œì íŠ¸ë¥¼ ì–´ë–»ê²Œ ì§„í–‰í•˜ë©´ ë ì§€ ìœ¤ê³½ì„ ì¡ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ","date":"2022-04-03","objectID":"/design/:1:0","tags":["k8s","AWS"],"title":"[k8s project] ì£¼ì œ ì •í•˜ê¸° ~ ì„¤ê³„","uri":"/design/"},{"categories":["project"],"content":"UI ì„¤ê³„ ì €ë‘ ë‹¤ë¥¸ íŒ€ì›ì´ ì•„í‚¤í…ì³ ì„¤ê³„ë¥¼ í•˜ëŠ” ë™ì•ˆ, ë‹¤ë¥¸ ë‘ íŒ€ì›ë¶„ë“¤ê»˜ì„œ ì›¹ UIì„¤ê³„ë¥¼ í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. (ë§í¬) íšŒê³  íŒ€ì„ ê¾¸ë ¤ ê°œë°œ í”„ë¡œì íŠ¸ë¥¼ í•˜ëŠ”ê²ƒì´ ì²˜ìŒì´ê³ , ê°œë°œ í”„ë¡œì íŠ¸ íŒ€ì¥ì„ ë§¡ëŠ” ê²ƒë„ ì²˜ìŒì´ë¼ ë¶€ë‹´ì´ ë§ì´ ë˜ê¸°ëŠ” í•©ë‹ˆë‹¤.ğŸ˜‚ ê·¸ë˜ë„ íŒ€ì¥ì´ ë˜ë‹ˆê¹Œ í”„ë¡œì íŠ¸ì— ì¢€ ë” ì—´ì‹¬íˆ ì°¸ì—¬í•˜ëŠ”ê±° ê°™ê³ , ë°°ìš°ëŠ”ê²ƒë„ ë” ë§ì´ ë°°ìš°ëŠ” ê²ƒ ê°™ì•„ ì„±ì·¨ê°ì´ ìˆëŠ”ê±° ê°™ìŠµë‹ˆë‹¤. ì´ì œ ë‹¤ìŒì£¼ë¶€í„°ëŠ” FEì„¤ê³„ë‘ BEì„¤ê³„ê°€ ë“¤ì–´ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤. íŒ€ ì—­í• ì„ ì •í•  ë•Œ íŒ€ì›ë“¤ë¼ë¦¬ ì—­í• ì„ ë‚˜ëˆŒ ë•Œ FE/BE ì´ëŸ°ì‹ìœ¼ë¡œ ë‚˜ëˆˆê²Œ ì•„ë‹ˆë¼ ì—…ë¬´ë³„ë¡œ ë‚˜ëˆ„ì–´ ì§„í–‰ì„ í•˜ê¸°ë¡œ ì •í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ Reactì™€ Springboot ëª¨ë‘ ë°°ìš°ë©´ì„œ ì§„í–‰ì„ í•˜ê²Œ ë  ì˜ˆì •ì¸ë°, ë§ì´ ë°”ì ê±° ê°™ì§€ë§Œ ë§ì´ ë°°ìš¸ ìˆ˜ ìˆì„ê±° ê°™ì•„ ê¸°ëŒ€ë©ë‹ˆë‹¤. ","date":"2022-04-03","objectID":"/design/:2:0","tags":["k8s","AWS"],"title":"[k8s project] ì£¼ì œ ì •í•˜ê¸° ~ ì„¤ê³„","uri":"/design/"},{"categories":["Algorithm"],"content":"Regular Expression ë³µì¡í•œ ë¬¸ìì—´ì„ ì²˜ë¦¬í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“  ê³³ì—ì„œ ì‚¬ìš© ","date":"2022-03-08","objectID":"/re/:0:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"ë©”íƒ€ë¬¸ì ","date":"2022-03-08","objectID":"/re/:1:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"[ ] ë¬¸ì í´ë˜ìŠ¤ [ ] ì‚¬ì´ì˜ ë¬¸ìë“¤ê³¼ ë§¤ì¹˜ - : ë‘ ë¬¸ì ì‚¬ì´ì˜ ë²”ìœ„ ^ : not 1. [abc] # a,b,cì¤‘ í•œê°œì˜ ë¬¸ìì™€ ë§¤ì¹­ 2. [a-c] # == [abc] 3. [a-zA-Z] # ëª¨ë“  ì•ŒíŒŒë²³ 4. [0-9] # ìˆ«ì re content \\d ìˆ«ìì™€ ë§¤ì¹˜, [0-9] \\D ìˆ«ìê°€ ì•„ë‹Œ ê²ƒê³¼ ë§¤ì¹˜, [^0-9] \\s whitespace ë¬¸ìì™€ ë§¤ì¹˜, [ \\t\\n\\r\\f\\v] \\S whitespace ë¬¸ìê°€ ì•„ë‹Œê²ƒê³¼ ë§¤ì¹˜, [^ \\t\\n\\r\\f\\v] \\w ë¬¸ì+ìˆ«ìì™€ ë§¤ì¹˜, [a-zA-Z0-9_] \\W ë¬¸ì+ìˆ«ìê°€ ì•„ë‹Œ ê²ƒê³¼ ë§¤ì¹˜, [^a-zA-Z0-9_] ","date":"2022-03-08","objectID":"/re/:1:1","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":". ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ë¬¸ì í´ë˜ìŠ¤(**[ ]**) ë‚´ì— Dot(**.**) ë©”íƒ€ ë¬¸ìê°€ ì‚¬ìš©ëœë‹¤ë©´ ì´ê²ƒì€ â€œëª¨ë“  ë¬¸ì\"ë¼ëŠ” ì˜ë¯¸ê°€ ì•„ë‹Œ ë¬¸ìÂ **.**Â ê·¸ëŒ€ë¡œë¥¼ ì˜ë¯¸ 1. a.b # a + [ëª¨ë“  ë¬¸ì] + b 2. a[.]b # a.b ","date":"2022-03-08","objectID":"/re/:1:2","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"*, +, {m,n} ë°˜ë³µ * : ë°”ë¡œ ì•ì— ìˆëŠ” ë¬¸ìê°€ 0ë¶€í„° ë¬´í•œëŒ€ë¡œ ë°˜ë³µë  ìˆ˜ ìˆìŒ + : ë°”ë¡œ ì•ì— ìˆëŠ” ë¬¸ìê°€ 1ë¶€í„° ë¬´í•œëŒ€ë¡œ ë°˜ë³µë  ìˆ˜ ìˆìŒ(ìµœì†Œ 1ë²ˆ) {m,n}: ë°”ë¡œ ì•ì— ìˆëŠ” ë¬¸ìê°€ më¶€í„° në²ˆê¹Œì§€ ë°˜ë³µë  ìˆ˜ ìˆìŒ(ìµœì†Œ më²ˆ, ìµœëŒ€ në²ˆ) ca*t # ct, cat,caat,caaaat, ... ca+t # cat, caat, caaat, ... ca{2}t # caat ca{2,4}t # caat, caaat, caaaat ","date":"2022-03-08","objectID":"/re/:1:3","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"? â€™{0, 1}â€™ ìˆì–´ë„ ë˜ê³ , ì—†ì–´ë„ ëœë‹¤ mol?lu # mollu, molu ","date":"2022-03-08","objectID":"/re/:1:4","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"| or ","date":"2022-03-08","objectID":"/re/:1:5","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"^, $,\\A,\\Z ^: ë¬¸ìì—´ì˜ ë§¨ ì²˜ìŒê³¼ ì¼ì¹˜, re.MULTILINEì‚¬ìš©ì‹œ ê° ì¤„ì˜ ì²˜ìŒê³¼ ì¼ì¹˜ $: ë¬¸ìì—´ ë§¨ ëê³¼ ì¼ì¹˜, re.MULTILINEì‚¬ìš©ì‹œ ê° ì¤„ì˜ ëê³¼ì¼ì¹˜ \\A: ì „ì²´ ë¬¸ìì—´ì˜ ë§¨ ì²˜ìŒê³¼ ì¼ì¹˜ \\Z: ì „ì²´ ë¬¸ìì—´ì˜ ë§¨ ëê³¼ ì¼ì¹˜ ","date":"2022-03-08","objectID":"/re/:1:6","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"\\b, \\B ë‹¨ì–´ êµ¬ë¶„ì \\b: whitespaceë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ì¸ ê²½ìš° ë§¤ì¹˜ \\B: whitespaceë¡œ êµ¬ë¶„ëœ ë‹¨ì–´ê°€ ì•„ë‹Œ ê²½ìš° ë§¤ì¹˜ ","date":"2022-03-08","objectID":"/re/:1:7","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"Grouping ( ) group(ì¸ë±ìŠ¤) ì„¤ëª… group(0) ë§¤ì¹˜ëœ ì „ì²´ ë¬¸ìì—´ group(1) ì²« ë²ˆì§¸ ê·¸ë£¹ì— í•´ë‹¹ë˜ëŠ” ë¬¸ìì—´ group(2) ë‘ ë²ˆì§¸ ê·¸ë£¹ì— í•´ë‹¹ë˜ëŠ” ë¬¸ìì—´ group(n) n ë²ˆì§¸ ê·¸ë£¹ì— í•´ë‹¹ë˜ëŠ” ë¬¸ìì—´ ","date":"2022-03-08","objectID":"/re/:2:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"module re íŒŒì´ì¬ì„ ì„¤ì¹˜í•  ë•Œ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë˜ëŠ” ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ Method ëª©ì  match() ë¬¸ìì—´ì˜ ì²˜ìŒë¶€í„° ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ”ì§€ ì¡°ì‚¬í•œë‹¤. search() ë¬¸ìì—´ ì „ì²´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ”ì§€ ì¡°ì‚¬í•œë‹¤. findall() ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ” ëª¨ë“  ë¬¸ìì—´(substring)ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒë ¤ì¤€ë‹¤. finditer() ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ” ëª¨ë“  ë¬¸ìì—´(substring)ì„ ë°˜ë³µ ê°€ëŠ¥í•œ ê°ì²´ë¡œ ëŒë ¤ì¤€ë‹¤. # match, search: trueë©´ ê°ì²´ ë°˜í™˜, falseë©´ None ë°˜í™˜ print(p.match(\"python\")) print(p.match(\"3 python\")) print(p.search(\"python\")) print(p.search(\"3 python\")) \u003e\u003e\u003e \u003cre.Match object; span=(0, 6), match='python'\u003e None \u003cre.Match object; span=(0, 6), match='python'\u003e \u003cre.Match object; span=(2, 8), match='python'\u003e -------- # match ê°ì²´ í™œìš© \u003e\u003e\u003e m = p.match(\"python\") \u003e\u003e\u003e m.group() # group: ë§¤ì¹˜ëœ ë¬¸ìì—´ ë°˜í™˜ 'python' \u003e\u003e\u003e m.start() # start: ë§¤ì¹˜ëœ ë¬¸ìì—´ì˜ ì‹œì‘ìœ„ì¹˜ ë°˜í™˜ 0 \u003e\u003e\u003e m.end() # end: ë§¤ì¹˜ëœ ë¬¸ìì—´ì˜ ëìœ„ì¹˜ ë°˜í™˜ 6 \u003e\u003e\u003e m.span() # ë§¤ì¹˜ëœ ë¬¸ìì—´ì˜ (ì‹œì‘, ë) ë°˜í™˜ (0, 6) print(p.findall(\"life is too short\")) for r in p.finditer(\"life is too short\"): print(r) \u003e\u003e\u003e ['life', 'is', 'too', 'short'] \u003cre.Match object; span=(0, 4), match='life'\u003e \u003cre.Match object; span=(5, 7), match='is'\u003e \u003cre.Match object; span=(8, 11), match='too'\u003e \u003cre.Match object; span=(12, 17), match='short'\u003e ","date":"2022-03-08","objectID":"/re/:3:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"ì»´íŒŒì¼ ì˜µì…˜ DOTALL(S) -Â .Â ì´ ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë“  ë¬¸ìì™€ ë§¤ì¹˜í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. \u003e\u003e\u003e p = re.compile('a.b', re.DOTALL) \u003e\u003e\u003e m = p.match('a\\nb') \u003e\u003e\u003e print(m) \u003cre.Match object; span=(0, 3), match='a\\nb'\u003e IGNORECASE(I) - ëŒ€ì†Œë¬¸ìì— ê´€ê³„ì—†ì´ ë§¤ì¹˜í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. \u003e\u003e\u003e p = re.compile('[a-z]+', re.I) \u003e\u003e\u003e p.match('python') \u003cre.Match object; span=(0, 6), match='python'\u003e \u003e\u003e\u003e p.match('Python') \u003cre.Match object; span=(0, 6), match='Python'\u003e \u003e\u003e\u003e p.match('PYTHON') \u003cre.Match object; span=(0, 6), match='PYTHON'\u003e MULTILINE(M) - ì—¬ëŸ¬ì¤„ê³¼ ë§¤ì¹˜í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. (^,Â $Â ë©”íƒ€ë¬¸ìì˜ ì‚¬ìš©ê³¼ ê´€ê³„ê°€ ìˆëŠ” ì˜µì…˜ì´ë‹¤) import re p = re.compile(\"^python\\s\\w+\", re.MULTILINE) data = \"\"\"python one life is too short python two you need python python three\"\"\" print(p.findall(data)) \u003e\u003e\u003e ['python one', 'python two', 'python three'] VERBOSE(X) - verbose ëª¨ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. (ì •ê·œì‹ì„ ë³´ê¸° í¸í•˜ê²Œ ë§Œë“¤ìˆ˜ ìˆê³  ì£¼ì„ë“±ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œëœë‹¤.) charref = re.compile(r\"\"\" \u0026[#] # Start of a numeric entity reference ( 0[0-7]+ # Octal form | [0-9]+ # Decimal form | x[0-9a-fA-F]+ # Hexadecimal form ) ; # Trailing semicolon \"\"\", re.VERBOSE) ","date":"2022-03-08","objectID":"/re/:4:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["Algorithm"],"content":"sub ì •ê·œì‹ê³¼ ë§¤ì¹˜ë˜ëŠ” ë¶€ë¶„ì„ ë‹¤ë¥¸ ë¬¸ìë¡œ ë³€ê²½ # [ì •ê·œì‹].sub(\"ë°”ê¿€ ë¬¸ìì—´\", \"ëŒ€ìƒ ë¬¸ìì—´\") \u003e\u003e\u003e p = re.compile('(blue|white|red)') \u003e\u003e\u003e p.sub('colour', 'blue socks and red shoes') 'colour socks and colour shoes' ","date":"2022-03-08","objectID":"/re/:5:0","tags":["Algorithm"],"title":"Regular Expression(ì •ê·œí‘œí˜„ì‹)","uri":"/re/"},{"categories":["project"],"content":"ì¡¸ì—…ë…¼ë¬¸ í”„ë¡œì íŠ¸(2)","date":"2022-01-13","objectID":"/3dprinting_printability_ml/","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"https://github.com/plooox/ML_3Dprinter_Scaffold ì €ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ì‘ì„±í–ˆë˜ ì´ë¯¸ì§€ ì²˜ë¦¬ë¶€ë¶„ì— ì´ì–´ì„œ, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„ ì •, í•™ìŠµ, ë¶„ì„ ê³¼ì •ì„ ê¸°ìˆ í•©ë‹ˆë‹¤. Machine Learning ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:0:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"ë¶„ë¥˜ ê¸°ë°˜ ë°©ë²•ë¡  ì‘ì„±í•œ ë…¼ë¬¸ì—ì„œëŠ” Printabilityê°€ ë†’ì€ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì–»ì„ ìˆ˜ ìˆëŠ”ê²ƒì„ ëª©í‘œë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ íŒŒë¼ë¯¸í„° ê°’ì„ ì§€ì •í•´ì£¼ëŠ” íšŒê·€ ëª¨ë¸ë³´ë‹¤ ìµœì  íŒŒë¼ë¯¸í„° ì¡°í•©ê³¼ íŒŒë¼ë¯¸í„° ë²”ìœ„ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë¶„ë¥˜ ëª¨ë¸ì´ ì í•©í•˜ê³  íŒë‹¨í•˜ì˜€ìŠµë‹ˆë‹¤. ë¶„ë¥˜ ëª¨ë¸ì€ Support Vector Machine(SVM)ê³¼ Random Forest(RF)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ë‘ê°œì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒì€ í‘œì¤€í¸ì°¨(Std), ì „ì²´ í”½ì…€ ìˆ˜(Sum)ì— ë”°ë¥¸ ë°ì´í„° ì…‹ì˜ ë¶„í¬ì…ë‹ˆë‹¤. ì´ë¥¼ í† ëŒ€ë¡œ ì ì ˆí•œ ì„ê³„ê°’ì„ ì§€ì •í•˜ì—¬ Printabilityì˜ â€œë‚®ìŒâ€ê³¼ â€œë†’ìŒâ€ì„ ë¼ë²¨ë§ í•˜ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ â€œì¢‹ìŒâ€, â€œë³´í†µâ€, â€œë‚˜ì¨â€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ 3x3 ë¶„í• ëœ 9ê°œì˜ Classë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:1:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Train Validation Test ë°ì´í„° ì‹ ë¢°ì„±ì„ ìœ„í•´ train validation testì— ê¸°ë°˜í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. í•™ìŠµ ì„¸íŠ¸(train set), ê²€ì¦ ì„¸íŠ¸(validation set), í‰ê°€ ì„¸íŠ¸(test set)ì— ê°ê° 60:20:20 ë¹„ìœ¨ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ í•™ìŠµì‹œì¼°ìŠµë‹ˆë‹¤. Validation setì„ training setìœ¼ë¡œ hold-out ê¸°ë°˜ìœ¼ë¡œ ë°˜ë³µ í•™ìŠµì‹œì¼°ê³ , ê°€ì¥ ì˜ í•™ìŠµëœ ëª¨ë¸ì„ ì„ ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  í•™ìŠµì‹œí‚¨ modelì— test setìœ¼ë¡œ testí•´ì„œ ìµœì¢…ì ì¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ , í•™ìŠµì´ ì˜ ë˜ì—ˆëŠ”ì§€ ë¶„ì„í•˜ëŠ” ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” RFì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¤‘ treeê°œìˆ˜ë¥¼ ì •í•˜ëŠ” n_estimators, SVM_RBFì˜ C, gammaê°’ì„ ì•Œê³ ë¦¬ì¦˜ì˜ ë§¤ê°œë³€ìˆ˜ë¡œ í•˜ì—¬ Testë¥¼ í†µí•´ ìµœì ì˜ ê°’ì„ ì°¾ì•„ í…ŒìŠ¤íŠ¸ í•˜ì˜€ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ê²°ê³¼ RFì—ì„œëŠ” n_estimatorê°€ 25ê°œ ì´ìƒì´ ë˜ì—ˆì„ ë•Œ, ì„±ëŠ¥ì´ í¬ê²Œ ì¦ê°€í•˜ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ n_estimatorê°’ì„ 61ë¡œ ì„¤ì •í•˜ê³ , accuracy_scoreì™€ f1_scoreë¥¼ ì¸¡ì •í•´ë³´ì•˜ìŠµë‹ˆë‹¤. RBF kernelì„ ì‚¬ìš©í•˜ëŠ” SVM ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” ì •ê·œí™” íŒŒë¼ë¯¸í„°ì¸ C(cost)ì™€ ê³¡ë¥  íŒŒë¼ë¯¸í„°ì¸ Gammaê°’ì„ ì¡°ì •í•˜ì—¬ ìµœì ì˜ ê²°ì • ê²½ê³„ë¥¼ ì¶œë ¥í•˜ëŠ” ê°’ì„ ì°¾ê³ ì í•˜ì˜€ìŠµë‹ˆë‹¤. íŠœë‹ ê²°ê³¼ gamma = 100, C = 35ë¥¼ íŒŒë¼ë¯¸í„° ê°’ìœ¼ë¡œ í•˜ì—¬ accuracy_scoreë¥¼ ì¸¡ì •í•˜ì˜€ìŠµë‹ˆë‹¤. ê²°ê³¼ ë° ë¶„ì„ ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:2:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Accuracy score RFì™€ SVMì˜ Accuracy_scoreë¥¼ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. ë‘ ëª¨ë¸ ëª¨ë‘ Validation Setê³¼ Test Setì˜ ì°¨ì´ê°€ í¬ê²Œ ë‚˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì•„, í•™ìŠµì´ ì–´ëŠì •ë„ ì¼ê´€ë˜ê²Œ ì˜ ì´ë£¨ì–´ì¡Œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  í•´ë‹¹ ì‹¤í—˜ì—ì„œëŠ” RFëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œì¼°ì„ ë•Œ, ì¢€ ë” ë†’ì€ ì •í™•ë„ê°€ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:3:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Parameter Set ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ RFëª¨ë¸ì„ í†µí•´, ì „ì²´ íŒŒë¼ë¯¸í„° ë²”ìœ„ì˜ ë°ì´í„°ì…‹ì„ ë„£ê³ , ë¶„ë¥˜í•´ë³´ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. í•™ìŠµëœ ëª¨ë¸ì´ ë°ì´í„°ì…‹ì„ ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë¶„ë¥˜í•œ ë°ì´í„° ì…‹ì—ì„œ â€œì¢‹ìŒâ€ì„ ë‚˜íƒ€ë‚´ëŠ” Class 1~3ì— ì†í•œ ì„ì˜ì˜ ë°ì´í„°ì™€ â€œë‚˜ì¨ì„ ë‚˜íƒ€ë‚´ëŠ” Class 7~9ì— ì†í•œ ì„ì˜ì— ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ì§ì ‘ ë‹¤ì‹œ í”„ë¦°íŒ… í•´ë³¸ ê²°ê³¼, ì‹¤ì œë¡œë„ â€œì¢‹ìŒâ€ê³¼ â€œë‚˜ì¨â€ì„ êµ¬ë³„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:4:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"Feature Importance RFëª¨ë¸ì˜ ê²½ìš°, feature_importance_ë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ SVMëª¨ë¸ì—ì„œë„ coef_ë¥¼ í†µí•´ ì¤‘ìš”ë„ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¸ê³ )https://stackoverflow.com/questions/41592661/determining-the-most-contributing-features-for-svm-classifier-in-sklearn ë…¸ì¦ ì˜¨ë„, ì¶œë ¥ ì†ë„, ë² ë“œ ì˜¨ë„ ìˆœìœ¼ë¡œ ì¤‘ìš”ë„ê°€ ë†’ë‹¤ëŠ” ê²°ê³¼ê°’ì´ ë‚˜ì˜¤ê²Œ ë˜ì—ˆëŠ”ë°, ì´ì²˜ëŸ¼ RFëª¨ë¸ì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´ 3D í”„ë¦°í„° ì‚¬ìš©ìê°€ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •ì‹œ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì£¼ì˜ê¹Šê²Œ ì¡°ì •í•´ì•¼í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_ml/:5:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ë¨¸ì‹ ëŸ¬ë‹","uri":"/3dprinting_printability_ml/"},{"categories":["project"],"content":"ì¡¸ì—…ë…¼ë¬¸ í”„ë¡œì íŠ¸(1)","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"ê°œìš” https://github.com/plooox/ML_3Dprinter_Scaffold ì¡¸ì—…ë…¼ë¬¸ì˜ ì¼í™˜ìœ¼ë¡œ íŒ€ì›ë“¤ê³¼ â€œë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3D í”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´â€ì´ë¼ëŠ” ì£¼ì œë¡œ ë…¼ë¬¸ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. ì €ëŠ” ëª‡ëª‡ íŒ€ì›ë“¤ê³¼ ì´ë¯¸ì§€ì²˜ë¦¬ë¶€ë¶„ê³¼ ë¨¸ì‹ ëŸ¬ë‹ íŒŒíŠ¸ë¥¼ ë§¡ì•„ ì§„í–‰í•˜ì˜€ëŠ”ë°, ê·¸ ì¤‘ì—ì„œ ì´ë¯¸ì§€ ì²˜ë¦¬ ê³¼ì •ì„ ê¸°ìˆ í•˜ê³ ì í•©ë‹ˆë‹¤. ë…¼ë¬¸ì˜ í•µì‹¬ì€ 3D í”„ë¦°íŒ…ì„ í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì„œ ë‚˜ì˜¤ëŠ” ì¶œë ¥ë¬¼ì´ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë§Œí¼ì˜ í’ˆì§ˆì„ ë‚¼ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì—¬ëŸ¬ íŒŒë¼ë¯¸í„°ë“¤ì˜ ì„¤ì •ê°’ ì¡°í•©ì„ ì¶”ì •í•˜ëŠ” ê³¼ì •ì„ ë¨¸ì‹ ëŸ¬ë‹ì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦í•˜ê³ , ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¼ì • ìˆ˜ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì§ì ‘ ì¶œë ¥í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ëª¨ë¸ì— í•™ìŠµì‹œí‚¤ê³ , í•™ìŠµëœ ëª¨ë¸ì´ ì¶”ì •í•œ ê°’ì´ ìœ íš¨í•œì§€ ê²€ì¦í•˜ì˜€ìŠµë‹ˆë‹¤. ì´ í˜ì´ì§€ì—ì„œëŠ” ì•„ë˜ ë¹¨ê°„ ë°•ìŠ¤ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶€ë¶„ì„ í¬ìŠ¤íŠ¸í•˜ë ¤ê³  í•©ë‹ˆë‹¤. 3D printing ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:0:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"ë„ë©´ ë„ë©´ì˜ ê²½ìš°, AUTOCAD Inventorí”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒ€ì›ì„ í†µí•´ ì–‡ì€ 1layerë¡œ êµ¬ì„±ëœ 500Î¼m ë°˜ì§€ë¦„ì„ ê°€ì§„ ì‘ì€ ì›í˜• pitì´ 100ê°œ ë‚˜ì—´ëœ micro-pit ëª¨ë¸ì„ ì„¤ê³„í•˜ì˜€ê³ , 3D printerë¥¼ ì´ìš©í•´ ì´ ë„ë©´ì„ í”„ë¦°íŒ… í–ˆìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:1:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Parameter selection ì‹¤í—˜ ì„¤ê³„ê³¼ì •ì—ì„œ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” ì¶œë ¥ ì†ë„, ë…¸ì¦ ì˜¨ë„, ë² ë“œ ì˜¨ë„ ì´ 3ê°€ì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì„ ì •í•´ ëœë¤ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ìƒì„±í•˜ê³  ì§ì ‘ í”„ë¦°íŠ¸í•˜ì—¬ ì´ 278ê°œì˜ ì…ë ¥ ë°ì´í„°ì…‹ì„ í˜•ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ë²”ìœ„ëŠ” ì „ì²´ì ì¸ ì¶œë ¥ í˜•íƒœë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ë³¸ ì‹¤í—˜ì—ì„œ ì‚¬ìš©í•œ 3D í”„ë¦°íŠ¸ì˜ ì¶œë ¥ì´ ë¶ˆê°€í•œ ë²”ìœ„ë¥¼ íŒŒì•…í•˜ê³  ì¶œë ¥ì´ ê°€ëŠ¥í•œ ë²”ìœ„ë¡œ í•œì •í•˜ì˜€ìŠµë‹ˆë‹¤. Parameter Range ì¶œë ¥ ì˜¨ë„ 180â„ƒ ~ 240â„ƒ ë² ë“œ ì˜¨ë„ 55â„ƒ ~ 85â„ƒ ì¶œë ¥ ì†ë„ 30mm/s ~ 150mm/s Image processing ì¶œë ¥í•œ ì¶œë ¥ë¬¼ì„ ì´ìš©í•´ ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ â€œì¢‹ì€ ì¶œë ¥ë¬¼â€ì„ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°í™” ì‹œí‚¤ëŠ” ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤. â€œì¢‹ì€ ì¶œë ¥ë¬¼\"ì€ ë„ë©´ê³¼ ìœ ì‚¬í•œ í¬ê¸°ì˜ ê· ì¼í•œ ê³µê·¹ì„ ê°€ì§€ëŠ” ì¶œë ¥ë¬¼ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì¸ PrintablilityëŠ” ì¶œë ¥ë¬¼ì˜ ê° ê³µê·¹ì˜ í”½ì…€ í•©ê³¼ í• ê°„ í‘œì¤€í¸ì°¨ë¥¼ í†µí•´ ì´ë£¨ì–´ ì¡ŒìŠµë‹ˆë‹¤. ê³µê·¹ì˜ í”½ì…€ í•©ì„ í†µí•´ ë„ë©´ê³¼ ì¶œë ¥ë¬¼ì˜ ìœ ì‚¬ì„±ì„ íŒë‹¨í•˜ê³ , í‘œì¤€í¸ì°¨ë¥¼ í†µí•´ ê· ì¼ë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í”½ì…€ í•©ì´ í¬ê³ , í‘œì¤€í¸ì°¨ ê°’ì´ ë‚®ì„ìˆ˜ë¡ Printabilityê°€ ë†’ë‹¤ê³  ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. Printabilityë¥¼ êµ¬í•˜ê¸° ìœ„í•´ í”„ë¦°í„°ì—ì„œ ì¶œë ¥í•œ ì´ë¯¸ì§€ë¥¼ ì´ë¯¸ì§€ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” Pythonê³¼ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ OpenCV(cv2)ë¥¼ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:2:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Preprocessing ì´ë¯¸ì§€ì²˜ë¦¬ ê³¼ì •ì„ ìë™í™” í•˜ê¸° ìœ„í•´, ì¶œë ¥ë¬¼ ì‚¬ì§„ì˜ ë°”ê¹¥ìª½ ë°”íƒ•ì„ ì œê±°í•˜ê³ , í‘/ë°±ìœ¼ë¡œ ì´ì§„í™” í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤. ìš°ì„ , ì´ë¯¸ì§€ì—ì„œ micro-pitsë¶€ë¶„ë§Œ ë½‘ì•„ë‚´ê¸° ìœ„í•´, Threshê°’ì„ ë°”ê¾¸ì–´ê°€ë©° ì´ë¯¸ì§€ë¥¼ ì´ì§„í™” í•´ë³´ì•˜ìŠµë‹ˆë‹¤. Threshê°’ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ì™¸ë¶€ ë…¸ì´ì¦ˆëŠ” ì¤„ì–´ë“¤ì§€ë§Œ, ë‚´ë¶€ ì´ë¯¸ì§€ê°€ ì˜ ë‚˜ì˜¤ì§€ ì•Šì•˜ê³ , Threshê°’ì„ ë‚®ì¶”ë©´ ì´ë¯¸ì§€ ìì²´ì˜ í€„ë¦¬í‹°ëŠ” ë†’ì§€ë§Œ ì™¸ë¶€ì— ë…¸ì´ì¦ˆê°€ ìƒê¸°ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Threshê°’ì„ ë†’ê²Œ ì¡ì•„ BoundingRectê°’ì„ ì¡ì•˜ì„ ë•Œ ì¶œë ¥ë¬¼ í…Œë‘ë¦¬ê°€ ëª…í™•íˆ ì¡íˆë„ë¡ í•˜ê³ , í…Œë‘ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ Cropí•˜ì—¬ ì¶œë ¥ë¬¼ë§Œ ë‚˜íƒ€ë‚˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒˆë¡œ êµ¬ì„±í•œ ë’¤, ì´ ì´ë¯¸ì§€ë¥¼ ë‚®ì€ Threshê°’ì—ì„œ ì´ì§„í™” í•˜ëŠ” ë°©ì‹ì„ ì·¨í•˜ì˜€ìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:3:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"Pixel Counting ì´í›„ Cropëœ ì´ë¯¸ì§€ë¥¼ 10 x 10ìœ¼ë¡œ êµ¬íšì„ ë‚˜ëˆ„ì–´ êµ¬íšë³„ ê²€ì • í”½ì…€ì˜ ìˆ˜ë¥¼ êµ¬í•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ ì „ì²´ ì´ë¯¸ì§€ì—ì„œ ê²€ì • í”½ì…€ ìˆ˜, í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•˜ì—¬ ë°ì´í„°ì…‹ìœ¼ë¡œ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤. ","date":"2022-01-13","objectID":"/3dprinting_printability_imageprocessing/:4:0","tags":["3D printing","ImageProcessing"],"title":"[Project] ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•œ 3Dí”„ë¦°íŒ…ì˜ printability ìµœì ì¡°ê±´ - ì´ë¯¸ì§€ ì²˜ë¦¬","uri":"/3dprinting_printability_imageprocessing/"},{"categories":["project"],"content":"NLP Project","date":"2022-01-06","objectID":"/sentimental_analysis_project/","tags":["NLP"],"title":"[Project] SNS ê°ì •ë¶„ì„ì„ í†µí•œ ì—¬ë¡  ì¡°ì‚¬","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"ê°œìš” https://github.com/plooox/SentimentalAnalysis ëŒ€í‘œì ì¸ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ì„œë¹„ìŠ¤ì¸ íŠ¸ìœ„í„°ì—ì„œ íŠ¹ì • í‚¤ì›Œë“œì˜ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•œ íŠ¸ìœ— ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ìˆ˜ì§‘í•œ íŠ¸ìœ— ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ì • ë¶„ì„ì„ ì‹¤ì‹œí•˜ì—¬ íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì—¬ë¡ ì´ ê¸ì •ì ì¸ì§€, ë¶€ì •ì ì¸ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” í”„ë¡œê·¸ë¨ ì…ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ íŠ¸ìœ„í„°ì˜ ê²½ìš°, íŠ¸ìœ„í„°apië¥¼ í†µí•´ íŠ¸ìœ„í„°ì˜ ë°ì´í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ íŠ¸ìœ„í„° apiì˜ ê²½ìš°, ë¬´ë£Œë¡œ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ 7ì¼ ì´ë‚´ì˜ ë°ì´í„°ë§Œ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. 7ì¼ì´ë¼ëŠ” ê¸°ê°„ì€ ë‚´ê°€ í•„ìš”ë¡œ í•˜ëŠ” ê¸°ê°„(6ê°œì›” ì´ìƒ)ì— ë¹„í•˜ë©´ í„°ë¬´ë‹ˆ ì—†ì´ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì—, ì™¸ë¶€ í”„ë¡œê·¸ë¨ì„ í†µí•´ ë°ì´í„°ë¥¼ í¬ë¡¤ë§ í•´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤. ì˜¤í”ˆì†ŒìŠ¤ë¡œ ì¡´ì¬í•˜ëŠ” ìˆ˜ë§ì€ í¬ë¡¤ëŸ¬ë“¤ ì¤‘ ëŒ€ë¶€ë¶„ì€ í¬ë¡¤ë§ì´ ì˜ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤(2021ë…„ 8ì›” ê¸°ì¤€). ê·¸ ì¤‘ snscrapeë¼ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ê°€ ì›í™œí•˜ê²Œ ë°ì´í„° ìˆ˜ì§‘ì´ ë˜ëŠ” ê²ƒì„ í™•ì¸í•˜ê³ , ì´ë¥¼ í†µí•´ ë°ì´í„° ìˆ˜ì§‘ì„ í•˜ì˜€ìŠµë‹ˆë‹¤. import numpy as np import pandas as pd import datetime as dt import re import pickle import csv import time import snscrape.modules.twitter as sntwitter start = \"2021-01-01\" end = \"2021-08-31\" dateTimeList = [] posNegList = [] contentList = [] tweetList = [] querys = [ #Keywords.... ] for query in querys: params = query+\"lang:ko \"+\"since:\"+start+\" until:\"+end csvRoute = \"./\"+query+\".csv\" csvFile = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") csvWrite.writerow([\"Date\",\"Pos/Neg\",\"Tweet\"]) csvFile.close() # for each keywords... for i,tweet in enumerate(sntwitter.TwitterSearchScraper(params).get_items()): # Twitter data -\u003e Date, Content # Save csv file tw_content = tweet.content tw_date = tweet.date tw_posNeg = -1 #Pos/Neg setting run another code! csvFile = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") csvWrite = csv.writer(csvFile) csvWrite.writerow([tw_date, tw_posNeg, tw_content]) csvFile.close() f = open(csvRoute, \"a\", newline='', encoding=\"utf-8\") r = csv.reader(f) cf = open('./Keyword1.csv', 'a', newline='', encoding='utf-8') wf = csv.writer(cf) wf.writerow([\"Date\",\"Pos/Neg\",\"Tweet\"]) f.close() cf.close() ê°ì • ë¶„ì„ ìˆ˜ì§‘í•œ íŠ¸ìœ„í„° ë°ì´í„°ë“¤ì„ ê°€ì§€ê³  ê°ì • ë¶„ì„ì„ í•˜ì—¬ íŠ¸ìœ— í…ìŠ¤íŠ¸ê°€ ê¸ì •ì ì¸ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³  ìˆëŠ”ì§€, ë¶€ì •ì ì¸ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³  ìˆëŠ”ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ kakao brainì—ì„œ ë°°í¬í•œ PORORO(Platform Of neuRal mOdels for natuRal language prOcessing)ë¥¼ ì´ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. POROROì—ì„œëŠ” ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ Taskë¥¼ ì§€ì›í•˜ëŠ”ë°, ì´ì¤‘ì—ì„œ â€œSentimental Analysisâ€ë¥¼ ì´ìš©í•˜ë©´ ì…ë ¥í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ â€˜ê¸ì •â€™ì¸ì§€, â€˜ë¶€ì •â€™ì¸ì§€ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. import pandas as pd import numpy as np import csv from pororo import Pororo sa = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") fileList = [#Keywords] # for each keywords.. for keyword in fileList: csvName = keyword+'_sa.csv' csvFile = open(csvName, 'a', newline='', encoding='utf-8') csvWrite = csv.writer(csvFile) csvWrite.writerow(['Date','Pos/Neg','Tweet']) csvFile.close() refCsv = pd.read_csv(keyword+'.csv') for idx, rowData in refCsv.iterrows(): tw_date = rowData['Date'] tw_content = rowData['Tweet'] tw_posneg = -1 try: if sa(tw_content) == \"Positive\": tw_posneg = 1 elif sa(tw_content) == \"Negative\": tw_posneg = 0 # Save csv file with Pos/Neg # Add for each row csvFile = open(csvName, 'a', newline='', encoding='utf-8') csvWrite = csv.writer(csvFile) csvWrite.writerow([tw_date, tw_posneg, tw_content]) csvFile.close() if (idx % 10000 == 0): # Check running print(\"Now: \"+ keyword+ \" - \"+str(idx)) except: print(\"error\") í‚¤ì›Œë“œ ì¶”ì¶œ ì´ì™• ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œ ê²¸, ì°¾ê³ ì í•˜ëŠ” í‚¤ì›Œë“œì™€ ì—°ê´€ë„ê°€ ë†’ì€ í‚¤ì›Œë“œë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤ê³  ìƒê°í•´ í‚¤ì›Œë“œ ì¶”ì¶œ ë¶€ë¶„ì„ ì¤‘ê°„ì— ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:0:0","tags":["NLP"],"title":"[Project] SNS ê°ì •ë¶„ì„ì„ í†µí•œ ì—¬ë¡  ì¡°ì‚¬","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"Noun - Corpus ìƒì„± ìš°ì„  ìˆ˜ì§‘í•œ íŠ¸ìœ— ë°ì´í„°ì—ì„œ, í‚¤ì›Œë“œê°€ ë  ìˆ˜ ìˆëŠ” ëª…ì‚¬ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤. ì•„ë¬´ë˜ë„ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ëŠ” í‚¤ì›Œë“œë¼ê³  ë¶€ë¥´ê¸° ì–´ë ¤ìš´ ë‹¨ì–´ë“¤ì´ ë§ì•„ì„œ, ì´ë¶€ë¶„ì„ ê±¸ëŸ¬ë‚´ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. í˜•íƒœì†Œ ë¶„ì„ê¸° ì¤‘ kkmaì—ì„œ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ëª…ì‚¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥ì´ ìˆì–´ì„œ ì´ë¥¼ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. import pandas as pd import numpy as np import pickle from konlpy.tag import Kkma pName = [#Keywords] for name in pName: df = pd.read_csv('#Path'+name+'.csv') tw = df['Tweet'] i = 0 twList = [] original = [] for twData in tw: noun = okt.nouns(twData) original.append(twData) string = \"\" for idx, v in enumerate(noun): if len(v) \u003c 2: noun.pop(idx) string = ' '.join(noun) twList.append(string) i += 1 if (i % 1000 == 0): print(\"Running....\") with open('#Path'+name+'.pkl', 'wb') as file: pickle.dump(twList, file) print(\"End: \"+name) with open('#Path'+name+'.pkl', 'wb') as file: pickle.dump(twList, file) ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:1:0","tags":["NLP"],"title":"[Project] SNS ê°ì •ë¶„ì„ì„ í†µí•œ ì—¬ë¡  ì¡°ì‚¬","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"í‚¤ì›Œë“œ ì¶”ì¶œ KR-WordRankëŠ” ë¹„ì§€ë„í•™ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•´ ìƒì„±í•œ Noun - Corpusì—ì„œ ìƒìœ„ 7ê°œì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤. from krwordrank.word import KRWordRank from krwordrank.sentence import summarize_with_sentences stopwords = {} # Stopwords you want to except penalty = lambda x:0 if (25 \u003c= len(x) \u003c= 80) else 1 keywords, sents = summarize_with_sentences( twList, penalty=penalty, # penalty of word-length: remove too short and too long stopwords = stopwords, diversity=0.5, # Diversity of keywords num_keywords=100, num_keysents=10, verbose=False ) pName = [#Keywords] for name in pName: df = pd.read_csv('#Path'+name+'.csv') tw = df['Tweet'] twList = [] nounStr = \"\" for twData in tw: noun = okt.nouns(twData) nounStr = ' '.join(noun) nounPos = kkma.pos(nounStr) string = \"\" onlyNNP = [] for (text, pos) in nounPos: if pos == 'NNP': onlyNNP.append(text) string = ' '.join(onlyNNP) twList.append(string) with open('#Path'+name+'_NNP.pkl', 'wb') as file: pickle.dump(twList, file) ì–´í”Œë¦¬ì¼€ì´ì…˜ ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì›¹ í˜ì´ì§€ë¥¼ êµ¬í˜„í•˜ê¸°ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. ì›¹í˜ì´ì§€ë¥¼ êµ¬ì„±í•œë‹¤ë©´ Django, Flaskë“±ì„ í™œìš©í•˜ì—¬ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ê²Œ ë˜ëŠ”ë°, ì €ëŠ” Streamlitì´ë¼ëŠ” íŒŒì´ì¬ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ íˆ´ì„ ì‚¬ìš©í•˜ê¸°ë¡œ í•˜ì˜€ìŠµë‹ˆë‹¤. Streamlitì€ ê¸°ê³„í•™ìŠµê³¼ ë°ì´í„° ê³¼í•™ì„ ìœ„í•œ ì›¹ ì•±ì„ í¸ë¦¬í•˜ê²Œ ë§Œë“¤ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. Streamlitì„ í†µí•´ ë¡œì»¬ í™˜ê²½ì—ì„œ ì›¹ ì–´í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ì–´ ì‹œì—°í•´ë³´ì•˜ìŠµë‹ˆë‹¤. í›„ê¸° ì‘ë…„ í•˜ë°˜ê¸°ì— ë§Œë“¤ì—ˆë˜ í”„ë¡œì íŠ¸ë¥¼ ë¬¸ì„œí™” í•˜ì˜€ìŠµë‹ˆë‹¤. ê³¼ì œë‚˜ í•™ìŠµì´ ì•„ë‹Œ í”„ë¡œì íŠ¸ë¥¼ ì²˜ìŒìœ¼ë¡œ í•´ë³´ê²Œ ë˜ì—ˆëŠ”ë°, ê°íšŒê°€ ìƒˆë¡œì› ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì„¤ê³„í•˜ê³ , êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ ë‚´ê°€ ëª¨ë¥´ëŠ” ê²ƒë“¤ì„ ì°¾ì•„ê°€ëŠ” ê²ƒì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë„ˆë¬´ ì™¸ë¶€ ì˜¤í”ˆì†ŒìŠ¤ì— ì˜ì¡´í•´ì„œ ë§Œë“¤ì—ˆë‹¤ëŠ” ì ì´ ì•„ì‰½ê²Œ ëŠê»´ì§€ê³ , ë§Œë“¤ë‹¤ë³´ë‹ˆ ê°œì„ í•  ì ë„ ë§ì´ ë‚˜ì˜¨ ê²ƒ ê°™ì•„ ì¢€ ì•„ì‰½ë‹¤ëŠ” ëŠë‚Œì´ ë“­ë‹ˆë‹¤. ","date":"2022-01-06","objectID":"/sentimental_analysis_project/:2:0","tags":["NLP"],"title":"[Project] SNS ê°ì •ë¶„ì„ì„ í†µí•œ ì—¬ë¡  ì¡°ì‚¬","uri":"/sentimental_analysis_project/"},{"categories":["project"],"content":"Colabí™˜ê²½ì—ì„œ Twitter í¬ë¡¤ë§ ","date":"2021-09-07","objectID":"/twitterapi/:0:0","tags":["NLP"],"title":"Colab í™˜ê²½ì—ì„œ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°","uri":"/twitterapi/"},{"categories":["project"],"content":"Twitter API ìƒì„± íŠ¸ìœ„í„° ê°€ì… ê°œë°œì ê³„ì • ë“±ë¡í•˜ê¸° APIìƒì„± ì°¸ê³ ) íŠ¸ìœ„í„° API ìŠ¹ì¸ë°›ê¸° ","date":"2021-09-07","objectID":"/twitterapi/:1:0","tags":["NLP"],"title":"Colab í™˜ê²½ì—ì„œ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°","uri":"/twitterapi/"},{"categories":["project"],"content":"íŠ¸ìœ— ê²€ìƒ‰ ë°ì´í„° ìˆ˜ì§‘ + Pororoë¥¼ í™œìš©í•œ ê°ì • ë¶„ì„ twitter_python twitter_consumer_key = \"\" twitter_consumer_secret = \"\" twitter_access_token = \"\" twitter_access_secret = \"\" import twitter twitter_api = twitter.Api(consumer_key=twitter_consumer_key, consumer_secret=twitter_consumer_secret, access_token_key=twitter_access_token, access_token_secret=twitter_access_secret) query = \" \" elements = twitter_api.GetSearch(term = query, count = 10000) pos = 0 pos_list = [] neg = 0 neg_list = [] senti_Analysis = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") for status in statuses: #print(senti_Analysis(status.text)) if (senti_Analysis(status.text) == \"Negative\"): neg += 1 neg_list.append(status.text) if (senti_Analysis(status.text) == \"Positive\"): pos += 1 pos_list.append(status.text) tweepy import tweepy from tweepy import Stream from tweepy import OAuthHandler from tweepy.streaming import StreamListener auth = tweepy.OAuthHandler(consumer_key = twitter_consumer_key, consumer_secret = twitter_consumer_secret) api = tweepy.API(auth) from pororo import Pororo query = \"KeyWord\" pos = 0 pos_list = [] neg = 0 neg_list = [] sa = Pororo(task = \"sentiment\", model = \"brainbert.base.ko.nsmc\", lang = \"ko\") res = [] for tw in tweepy.Cursor(api.search, q = query, since = \"2021-07-14\").items(): tw_txt = tw.text if sa(tw_txt) == \"Positive\": pos += 1 pos_list.append(tw_txt) if sa(tw_txt) == \"Negative\": neg += 1 neg_list.append(tw_txt) res.append(tw_txt) print(pos) print(neg) ","date":"2021-09-07","objectID":"/twitterapi/:2:0","tags":["NLP"],"title":"Colab í™˜ê²½ì—ì„œ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°","uri":"/twitterapi/"},{"categories":["project"],"content":"Twitter APIì˜ í•œê³„ 1ì£¼ì „ íŠ¸ìœ—ê¹Œì§€ë§Œ ì ‘ê·¼ ê°€ëŠ¥ ì¿¼ë¦¬ ì†ë„ ì œí•œ : 5ë¶„ë‹¹ 180ì¿¼ë¦¬ ","date":"2021-09-07","objectID":"/twitterapi/:2:1","tags":["NLP"],"title":"Colab í™˜ê²½ì—ì„œ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°","uri":"/twitterapi/"},{"categories":["project"],"content":"twint import twint c = twint.Config() c.Search = 'í˜ì´ì»¤' c.Limit = 5 c.Since = '2021-09-13' c.Until = '2021-09-14' c.Output = \"test.json\" c.Popular_tweets = False c.Store_json = True twint.run.Search(c) ìµëª…ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥ ê±°ì˜ ëª¨ë“  íŠ¸ìœ— ìˆ˜ì§‘ ê°€ëŠ¥ ì†ë„ ì œí•œ ì—†ìŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì„±ëŠ¥ì´ ë§¤ìš° ì¢‹ì§€ ëª»í•˜ì˜€ìŒ ì°¸ê³ ) íŒŒì´ì¬ê³¼ íŠ¸ìœ„í„° APIë¥¼ í™œìš©í•œ íŠ¸ìœ„í„° í¬ë¡¤ë§ íŠ¸ìœ„í„° íŒŒí—¤ì¹˜ê¸° ì‹œë¦¬ì¦ˆ ì²«ë²ˆì§¸ - ìˆ˜ì§‘í•˜ê¸° ","date":"2021-09-07","objectID":"/twitterapi/:3:0","tags":["NLP"],"title":"Colab í™˜ê²½ì—ì„œ íŠ¸ìœ— ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°","uri":"/twitterapi/"},{"categories":["NLP"],"content":"í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ NLP ê³µë¶€","date":"2021-08-24","objectID":"/nlp_05/","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"Splitting Data ","date":"2021-08-24","objectID":"/nlp_05/:0:0","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"Superivised Learning(ì§€ë„í•™ìŠµ) ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì…ë ¥ê°’(X_Label)ê³¼ ê²°ê³¼ê°’(Y_Label)ì´ ìˆëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ","date":"2021-08-24","objectID":"/nlp_05/:1:0","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"X_Labelê³¼ Y_Label ë¶„ë¦¬í•˜ê¸° ","date":"2021-08-24","objectID":"/nlp_05/:2:0","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"zipí•¨ìˆ˜ sequence = [['a', 1], ['b', 2], ['c', 3]] x,y = zip(*sequence) #ë°ì´í„° unpackìœ„í•´ *ì‚¬ìš© print(x) print(y) ('a', 'b', 'c') (1, 2, 3) ","date":"2021-08-24","objectID":"/nlp_05/:2:1","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"DataFrame import pandas as pd values = [['ë‹¹ì‹ ì—ê²Œ ë“œë¦¬ëŠ” ë§ˆì§€ë§‰ í˜œíƒ!', 1], ['ë‚´ì¼ ëµ ìˆ˜ ìˆì„ì§€ í™•ì¸ ë¶€íƒë“œ...', 0], ['ë„ì—°ì”¨. ì˜ ì§€ë‚´ì‹œì£ ? ì˜¤ëœë§Œì…...', 0], ['(ê´‘ê³ ) AIë¡œ ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤!', 1]] columns = ['ë©”ì¼ ë³¸ë¬¸', 'ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´'] df = pd.DataFrame(values, columns=columns) x_label = df['ë©”ì¼ ë³¸ë¬¸'] y_label = df['ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´'] print(\"[X_Label]\") print(x_label) print(\"\\n[Y_Label]\") print(y_label) [X_Label] 0 ë‹¹ì‹ ì—ê²Œ ë“œë¦¬ëŠ” ë§ˆì§€ë§‰ í˜œíƒ! 1 ë‚´ì¼ ëµ ìˆ˜ ìˆì„ì§€ í™•ì¸ ë¶€íƒë“œ... 2 ë„ì—°ì”¨. ì˜ ì§€ë‚´ì‹œì£ ? ì˜¤ëœë§Œì…... 3 (ê´‘ê³ ) AIë¡œ ì£¼ê°€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤! Name: ë©”ì¼ ë³¸ë¬¸, dtype: object [Y_Label] 0 1 1 0 2 0 3 1 Name: ìŠ¤íŒ¸ ë©”ì¼ ìœ ë¬´, dtype: int64 ","date":"2021-08-24","objectID":"/nlp_05/:2:2","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ xì™€ yê°€ ì´ë¯¸ ë¶„ë¦¬ëœ ë°ì´í„°ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ ê³¼ì • ","date":"2021-08-24","objectID":"/nlp_05/:3:0","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"sklearn train_test_splitì„ í™œìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=42) x: ë…ë¦½ ë³€ìˆ˜ ë°ì´í„°(ë°°ì—´, ë°ì´í„° í”„ë ˆì„) y: ì¢…ì† ë³€ìˆ˜ ë°ì´í„°, ë ˆì´ë¸” ë°ì´í„° test_size: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ê°œìˆ˜, 1ë³´ë‹¤ ì‘ì„ ì‹œ ë¹„ìœ¨ ì˜ë¯¸ train_size: í•™ìŠµìš© ë°ì´í„°ì˜ ê°œìˆ˜, 1ë°”ë„ ì‘ì„ ì‹œ ë¹„ìœ¨ ì˜ë¯¸ random_state: ë‚œìˆ˜ ì‹œë“œ import numpy as np from sklearn.model_selection import train_test_split x,y = np.arange(10).reshape((5,2)), range(5) #ë¶„ë¦¬ëœ x,y ë°ì´í„° ìƒì„± y = list(y) print(x) print(y) print(\"\\n\\n\") x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 1234) # ì „ì²´ ë°ì´í„° ì¤‘ 1/3ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì§€ì • # random_stateì˜ seed_value = 42 print(\"[X_train]\") print(x_train) print(\"[X_test]\") print(x_test) print(\"\\n[Y_train]\") print(y_train) print(\"[Y_test]\") print(y_test) [[0 1] [2 3] [4 5] [6 7] [8 9]] [0, 1, 2, 3, 4] [X_train] [[2 3] [4 5] [6 7]] [X_test] [[8 9] [0 1]] [Y_train] [1, 2, 3] [Y_test] [4, 0] ì°¸ê³ ) ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ","date":"2021-08-24","objectID":"/nlp_05/:3:1","tags":["NLP"],"title":"[NLP] 5.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - Splitting Data","uri":"/nlp_05/"},{"categories":["NLP"],"content":"í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ NLP ê³µë¶€","date":"2021-08-18","objectID":"/nlp_04/","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"ì •ìˆ˜ ì¸ì½”ë”© ì»´í“¨í„°ëŠ” í…ìŠ¤íŠ¸ë³´ë‹¤ ìˆ«ìë¥¼ ë” ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ì´ íŠ¹ì§•ì„ ì‚´ë¦¬ê¸° ìœ„í•´ NLPì—ì„œëŠ” í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê¸°ë²•ë“¤ì„ ì œì•ˆí•œë‹¤. ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì€ ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ê°’ì— Mappingì‹œí‚¤ëŠ” ì „ì²˜ë¦¬ ì‘ì—…ì´ ìš”êµ¬ë˜ê¸°ë„ í•œë‹¤. ","date":"2021-08-18","objectID":"/nlp_04/:0:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Dictionary from nltk.tokenize import sent_tokenize from nltk.tokenize import word_tokenize from nltk.corpus import stopwords text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\" text = sent_tokenize(text) #ë¬¸ì¥ í† í°í™” vocab = {} sentence = [] stop_words = set(stopwords.words('english')) for i in text: s = word_tokenize(i) # ë¬¸ì¥ì— ëŒ€í•´ ë‹¨ì–´ í† í°í™” res = [] for w in s: w = w.lower() #ëŒ€ë¬¸ì -\u003e ì†Œë¬¸ì if w not in stop_words: #ë¶ˆìš©ì–´ ì œê±° if len(w) \u003e 2: # ë‹¨ì–´ ê¸¸ì´ê°€ 2ì´í•˜ì¸ ê²½ìš° ì œê±° res.append(w) if w not in vocab: vocab[w] = 0 vocab[w] += 1 sentence.append(res) print(\"Word Tokenize\") print(sentence) # ë‹¨ì–´í† í°í™” print(\"Word Frequency\") print(vocab) # ì¤‘ë³µì„ ì œê±°í•œ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse= True) #ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ print(vocab_sorted) word_to_idx = {} #ë¹ˆë„ìˆ˜ ë†’ì€ ìˆœì„œëŒ€ë¡œ ê³ ìœ ë²ˆí˜¸ ë¶€ì—¬ idx = 0 for (word, freq) in vocab_sorted: if freq \u003e 1: idx += 1 word_to_idx[word] = idx print(\"Encoded Word\") print(word_to_idx) Word Tokenize [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']] Word Frequency {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1} [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)] Encoded Word {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7} word_to_idxë¥¼ ì´ìš©í•˜ì—¬ í† í°í™”ëœ sentenceì˜ ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ex) [â€˜barberâ€™, â€˜personâ€™] -\u003e [1,5] ë‹¨ì–´ ì§‘í•©ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë“¤ì€ Out-Of-Vocaë¼ê³  í•˜ë©°, ìƒˆë¡œìš´ indexë¥´ ì¸ì½”ë”©í•œë‹¤. word_to_idx['OOV'] = len(word_to_idx) + 1 ","date":"2021-08-18","objectID":"/nlp_04/:0:1","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Counter from collections import Counter words = sum(sentence, []) #ë‹¨ì–´ì§‘í•©ì„ ë§Œë“¤ê¸° ìœ„í•´ ë¬¸ì¥ì˜ ê²½ê³„ì¸ []ì œê±° vocab = Counter(words) #ì¤‘ë³µì„ ì œê±°í•˜ê³  ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ê¸°ë¡ vocab = vocab.most_common(5) #ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œì˜ ë‹¨ì–´ë§Œ ì €ì¥ print(vocab) [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)] ","date":"2021-08-18","objectID":"/nlp_04/:0:2","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"NLTK - FreqDist Counter()ì™€ ì‚¬ìš©ë°©ë²•ì´ ë™ì¼ from nltk import FreqDist import numpy as np vocab = FreqDist(np.hstack(sentence)) #ë¬¸ì¥ êµ¬ë¶„ ì œê±° == sum(sentence, []) print(vocab.most_common(5)) [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)] ","date":"2021-08-18","objectID":"/nlp_04/:0:3","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"enumerate enumerate: ìˆœì„œê°€ ìˆëŠ” ìë£Œí˜•(list, set, tuple,dictionary, string)ì„ ì…ë ¥ë°›ì•„ ì¸ë±ìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¦¬í„´ word_to_index = {word[0]: index+1 for index, word in enumerate(vocab)} print(word_to_idx) {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7} ","date":"2021-08-18","objectID":"/nlp_04/:0:4","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Keras KerasëŠ” ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬ ë„êµ¬ë¥¼ ì œê³µ ì •ìˆ˜ ì¸ì½”ë”©ì˜ ê²½ìš°, Kerasì˜ Tokenizerë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. from tensorflow.keras.preprocessing.text import Tokenizer tokenizer = Tokenizer() tokenizer.fit_on_texts(sentences) #sentences: ë‹¨ì–´ í† í°í™”ëœ ë°ì´í„° print(\"[Word index]\") # ì¸ë±ìŠ¤ ì¶œë ¥ print(tokenizer.word_index) print(\"[Word count]\") # ì¹´ìš´íŠ¸ ì¶œë ¥ print(tokenizer.word_counts) print(\"[sequences]\") # corpusë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ print(tokenizer.texts_to_sequences(sentence)) [Word index] {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13} [Word count] OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]) [sequences] [[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]] Keras TokenizerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ OOVì— ëŒ€í•´ ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ë‹¨ì–´ë¥¼ ì œê±°í•œë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ OOVë¥¼ ë³´ì¡´í•˜ê³  ì‹¶ìœ¼ë©´ ì¸ì oov_tokenì„ ì‚¬ìš©í•œë‹¤. voca_size = 5 # ë¹ˆë„ìˆ˜ ìƒìœ„ 5ê°œ ë‹¨ì–´ë§Œ ì‚¬ìš©, OOVì™€ ìˆ«ì 0 ê³ ë ¤í•˜ì—¬ í¬ê¸°ëŠ” +2 tokenizer = Tokenizer(num_words= voca_size + 2, oov_token='OOV') tokenizer.fit_on_texts(sentences) print(tokenizer.texts_to_sequences(sentences)) [[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]] Padding ê¸°ê³„ëŠ” ê¸¸ì´ê°€ ë™ì¼í•œ ë¬¸ì„œì— ëŒ€í•´ í–‰ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥ ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•´ì„œ ì—¬ëŸ¬ ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶°ì£¼ëŠ” ì‘ì—… ë°ì´í„°ì— íŠ¹ì • ê°’ì„ ì±„ì›Œì„œ ë°ì´í„°ì˜ í¬ê¸°(shape)ë¥¼ ì¡°ì •í•˜ëŠ” ì‘ì—… ","date":"2021-08-18","objectID":"/nlp_04/:1:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Numpy ì‚¬ìš© import numpy as np max_len = max(len(item) for item in encoded) #ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ êµ¬í•˜ê¸° for item in encoded: while len(item) \u003c max_len: item.append(0) # ìˆ«ì 0 ì‚¬ìš© -\u003e Zero padding padded_np = np.array(encoded) print(padded_np) [[ 1 5 0 0 0 0 0] [ 1 8 5 0 0 0 0] [ 1 3 5 0 0 0 0] [ 9 2 0 0 0 0 0] [ 2 4 3 2 0 0 0] [ 3 2 0 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 2 0 0 0 0] [ 7 7 3 2 10 1 11] [ 1 12 3 13 0 0 0]] ","date":"2021-08-18","objectID":"/nlp_04/:2:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"keras ì‚¬ìš© from tensorflow.keras.preprocessing.sequence import pad_sequences encoded = tokenizer.texts_to_sequences(sentences) pad_seq = pad_sequences(encoded) print(pad_seq) [[ 1 5 0 0 0 0 0] [ 1 8 5 0 0 0 0] [ 1 3 5 0 0 0 0] [ 9 2 0 0 0 0 0] [ 2 4 3 2 0 0 0] [ 3 2 0 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 6 0 0 0 0] [ 1 4 2 0 0 0 0] [ 7 7 3 2 10 1 11] [ 1 12 3 13 0 0 0]] ","date":"2021-08-18","objectID":"/nlp_04/:3:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"ê¸¸ì´ ì œí•œ, ê°’ ìˆ˜ì • # ê¸¸ì´ì œí•œ: maxlen = 5 # value ìˆ˜ì •: ì‚¬ìš©ëœ ì •ìˆ˜ì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ ë‹¨ì–´ ì§‘í•© í¬ê¸°ë³´ë‹¤ 1 í° ìˆ˜ ì‚¬ìš© padded = pad_sequences(encoded, maxlen=5, padding='post', value = len(tokenizer.word_index)+1) print(padded) # ê¸¸ì´ê°€ maxlen(= 5)ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ íŒ¨ë”©, í¬ë©´ ë°ì´í„° ì†Œë©¸ [[ 1 5 14 14 14] [ 1 8 5 14 14] [ 1 3 5 14 14] [ 9 2 14 14 14] [ 2 4 3 2 14] [ 3 2 14 14 14] [ 1 4 6 14 14] [ 1 4 6 14 14] [ 1 4 2 14 14] [ 3 2 10 1 11] [ 1 12 3 13 14]] One-Hot Encoding ","date":"2021-08-18","objectID":"/nlp_04/:3:1","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Vocabulary(ë‹¨ì–´ì§‘í•©) ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì§‘í•© í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•Šê³  ëª¨ì•„ë†“ì„ ê²ƒì„ ì§€ì¹­ One-Hot encodingì€ ë‹¨ì–´ ì§‘í•©ì„ ë§Œë“¤ê³  ê° ë‹¨ì–´ì— ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•˜ëŠ” ê²ƒì—ì„œë¶€í„° ì‹œì‘ëœë‹¤. ","date":"2021-08-18","objectID":"/nlp_04/:4:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"One-Hot Encoding ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë°±í„°ì˜ ì°¨ì›ìœ¼ë¡œ í•˜ê³ , í‘œí˜„í•˜ê³ ì í•˜ëŠ” ë‹¨ì–´ì˜ ì¸ë±ìŠ¤ì— 1ì˜ ê°’ì„ ë¶€ì—¬í•˜ëŠ” ë‹¨ì–´ì˜ ë²¡í„° í‘œí˜„ ë°©ì‹. ì´ë ‡ê²Œ í‘œí˜„ëœ ë²¡í„°ë¥¼ One-Hot vectorë¼ê³  í•œë‹¤. ","date":"2021-08-18","objectID":"/nlp_04/:5:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"Keras í™œìš© from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.utils import to_categorical text = \"ë‚˜ë‘ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ì ì‹¬ ë©”ë‰´ëŠ” í–„ë²„ê±° ê°ˆë˜ ê°ˆë˜ í–„ë²„ê±° ìµœê³ ì•¼\" sub_text=\"ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°ˆë˜ ë©”ë‰´ëŠ” í–„ë²„ê±° ìµœê³ ì•¼\" # 1. ì •ìˆ˜ ì¸ì½”ë”© t = Tokenizer() t.fit_on_texts([text]) # ê° ë‹¨ì–´ì— ëŒ€í•œ ì¸ì½”ë”© print(t.word_index) encoded = t.texts_to_sequences([sub_text])[0] #[[1,2,3,4]] -\u003e [1,2,3,4] print(encoded) # 2. One-Hot vector ìƒì„± one_hot = to_categorical(encoded) print(one_hot) {'ê°ˆë˜': 1, 'ì ì‹¬': 2, 'í–„ë²„ê±°': 3, 'ë‚˜ë‘': 4, 'ë¨¹ìœ¼ëŸ¬': 5, 'ë©”ë‰´ëŠ”': 6, 'ìµœê³ ì•¼': 7} #ë‹¨ì–´ ì¸ë±ìŠ¤ [2, 5, 1, 6, 3, 7] # ì •ìˆ˜ ì¸ì½”ë”© [[0. 0. 1. 0. 0. 0. 0. 0.] # ì›-í•« ì¸ì½”ë”© [0. 0. 0. 0. 0. 1. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1.]] ","date":"2021-08-18","objectID":"/nlp_04/:6:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"í•œê³„ì  ë‹¨ì–´ì˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡, ë²¡í„°ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ê³µê°„ì´ ëŠ˜ì–´ë‚œë‹¤. ë‹¨ì–´ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•  ìˆ˜ ì—†ìŒ ex) bookê³¼ booksë¥¼ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¼ê³  ì·¨ê¸‰ ì°¸ê³ ) ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ","date":"2021-08-18","objectID":"/nlp_04/:7:0","tags":["NLP"],"title":"[NLP] 4.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì¸ì½”ë”© \u0026 íŒ¨ë”©","uri":"/nlp_04/"},{"categories":["NLP"],"content":"í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ NLP ê³µë¶€","date":"2021-08-17","objectID":"/nlp_03/","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Cleaning and Normalization Tokenization ì „, í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìš©ë„ì— ë§ê²Œ ì •ì œ(cleaning)ë° ì •ê·œí™”(normalization)í•˜ëŠ” ì¼ ì •ì œ(cleaning): ê°€ì§€ê³  ìˆëŠ” corpusë¡œë¶€í„° noise dataì œê±° ì •ê·œí™”(normalization): í‘œí˜„ ë°©ë²•ì´ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ í†µí•©, ê°™ì€ ë‹¨ì–´ë¡œ ì¬êµ¬ì„± ","date":"2021-08-17","objectID":"/nlp_03/:0:0","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ê·œì¹™ì— ê¸°ë°˜í•œ í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ í†µí•© ê°™ì€ ì˜ë¯¸ë¥¼ ê°–ê³  ìˆëŠ” í‘œê¸°ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì •ê·œí™” ","date":"2021-08-17","objectID":"/nlp_03/:1:0","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Lemmatization(í‘œì œì–´ ì¶”ì¶œ) ë‹¨ì–´ë“¤ë¡œë¶€í„° í‘œì œì–´ë¥¼ ì°¾ì•„ê°€ëŠ” ê³¼ì • ex) am, are, is -\u003e be í˜•íƒœí•™ì  íŒŒì‹± ë‹¨ì–´ì˜ ì–´ê°„(stem, ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” í•µì‹¬ ë¶€ë¶„)ê³¼ ì ‘ì‚¬(affix, ë‹¨ì–´ì˜ ì¶”ê°€ì ì¸ ì˜ë¯¸ ë¶€ì—¬)ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì‘ì—… NLTKì—ì„œëŠ” WorkNetLemmatizerë¥¼ ì§€ì› from nltk.stem import WordNetLemmatizer n=WordNetLemmatizer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([n.lemmatize(w) for w in words]) ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting'] # 'dy', 'ha': ì ì ˆí•˜ì§€ ëª»í•œ ë‹¨ì–´ # lemmatizerëŠ” ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ë¥¼ ì•Œì•„ì•¼ë§Œ ì •í™•í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•œë‹¤. # WordNetLemmatizerëŠ” ì…ë ¥ìœ¼ë¡œ ë‹¨ì–´ì˜ í’ˆì‚¬ ì •ë³´ ì œê³µ ê°€ëŠ¥ ex) n.lemmatize('has', 'v') -\u003e ë‹¨ì–´ì˜ í˜•íƒœê°€ ì–´ëŠì •ë„ ë³´ì¡´ë˜ëŠ” íŠ¹ì§•ì„ ê°€ì§„ë‹¤. ","date":"2021-08-17","objectID":"/nlp_03/:1:1","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Stemming(ì–´ê°„ ì¶”ì¶œ) ë‹¨ì–´ì—ì„œ ì–´ê°„(stem)ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì • ì„¬ì„¸í•œ ì‘ì—…ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ì–´ê°„ì¶”ì¶œí›„ì— ë‚˜ì˜¤ëŠ” ê²°ê³¼ëŠ” ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ì¸ ê²½ìš°ë„ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í‘œì œì–´ ì¶”ì¶œë³´ë‹¤ ë¹ ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ë‹¤. NLTKì—ì„œëŠ” Porter stemmerì™€ Lancaster Stemmerë¥¼ ì§€ì›í•œë‹¤. #Porter Stemmer from nltk.stem import PorterStemmer s=PorterStemmer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([s.stem(w) for w in words]) ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start'] from nltk.stem import LancasterStemmer l=LancasterStemmer() words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([l.stem(w) for w in words]) ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start'] -\u003e Stemmer ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ë™ì¼í•œ ë‹¨ì–´ì—ì„œë„ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê¸° ë•Œë¬¸ì—, ì–´ë–¤ Stemmerê°€ ì í•©í•œì§€ íŒë‹¨í•˜ê³  ì‚¬ìš©í•´ì•¼í•œë‹¤. ","date":"2021-08-17","objectID":"/nlp_03/:1:2","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ëŒ€, ì†Œë¬¸ì í†µí•© ì˜ì–´ê¶Œì—ì„œ ëŒ€ë¬¸ìì˜ ê²½ìš°, íŠ¹ì • ìƒí™©ì—ì„œë§Œ ì“°ì´ê¸° ë•Œë¬¸ì—, ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°˜í™˜í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ","date":"2021-08-17","objectID":"/nlp_03/:2:0","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° Noise data: ì•„ë¬´ ì˜ë¯¸ë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” ê¸€ì(íŠ¹ìˆ˜ë¬¸ì) + ë¶„ì„í•˜ê³ ì í•˜ëŠ” ëª©ì ì— ë§ì§€ ì•ŠëŠ” ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ","date":"2021-08-17","objectID":"/nlp_03/:3:0","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ë¶ˆìš©ì–´ ì œê±° ë¶ˆìš©ì–´(Stopword): ë¬¸ì¥ì˜ ì‹¤ì œ ì˜ë¯¸ ë¶„ì„ì„ í•˜ëŠ”ë° ë„ì›€ë˜ì§€ ì•ŠëŠ” ë‹¨ì–´(I, my, me, over â€¦) ì§ì ‘ ë¶ˆìš©ì–´ë¥¼ ì •ì˜í•´ì„œ ì‚¬ìš©í•˜ê±°ë‚˜, íŒ¨í‚¤ì§€ì—ì„œ ì •ì˜ëœ ë¶ˆìš©ì–´ë¥¼ ì‚¬ìš© NLTKì—ì„œëŠ” 100ì—¬ê°œ ì´ìƒì˜ ë‹¨ì–´ë¥¼ ë¶ˆìš©ì–´ë¡œ íŒ¨í‚¤ì§€ ë‚´ì—ì„œ ì •ì˜í•˜ê³  ìˆë‹¤. from nltk.corpus import stopwords from nltk.tokenize import word_tokenize example = \"Family is not an important thing. It's everything.\" stop_words = set(stopwords.words('english')) word_tokens = word_tokenize(example) result = [] for w in word_tokens: if w not in stop_words: # ë‹¨ì–´ê°€ ë¶ˆìš©ì–´ê°€ ì•„ë‹ˆë©´ ì¶”ê°€ result.append(w) ","date":"2021-08-17","objectID":"/nlp_03/:3:1","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ë“±ì¥ ë¹ˆë„ê°€ ì ì€ ë‹¨ì–´ ì œê±° í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë„ˆë¬´ ì ê²Œ ë“±ì¥í•˜ì—¬ ìì—°ì–´ ì²˜ë¦¬ì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë“¤ì„ ì œê±°. ","date":"2021-08-17","objectID":"/nlp_03/:3:2","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"ê¸¸ì´ê°€ ì§§ì€ ë‹¨ì–´ ex) 100000ê°œì˜ ë©”ì¼ ë°ì´í„°ì—ì„œ 3~4ë²ˆ ë°–ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ì œê±° ì˜ì–´ê¶Œ ì–¸ì–´ì˜ ê²½ìš° ê¸¸ì´ê°€ ì§§ì€ ì–¸ì–´ë“¤ì„ ì œê±°í•˜ëŠ” ê²ƒì€ ë¬¸ì¥ì—ì„œ í¬ê²Œ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê¸¸ì´ê°€ 2~3 ì´í•˜ì¸ ë‹¨ì–´ë“¤ì„ ì œê±°í•˜ëŠ” ê²ƒì€ ë§ì€ ìˆ˜ì˜ ë¶ˆìš©ì–´(it, at, on â€¦)ë“¤ì„ ì œê±°í•˜ëŠ”ë° íš¨ê³¼ì ì´ë‚˜, ê¸¸ì´ê°€ ì§§ì€ ëª…ì‚¬ë“¤(ox, fox, cat â€¦) ë˜í•œ ì œê±°ë  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ","date":"2021-08-17","objectID":"/nlp_03/:3:3","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"Regular Expression íŠ¹ì •í•œ ê·œì¹™ì„ ê°€ì§„ ë¬¸ìì—´ì˜ ì§‘í•©ì„ í‘œí˜„í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” í˜•ì‹ ì–¸ì–´. ë¬¸ìì—´ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²• ì¤‘ì˜ í•˜ë‚˜ë¡œ, ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ë©´ íŠ¹ì •í•œ ì¡°ê±´ì˜ ë¬¸ìë¥¼ â€˜ê²€ìƒ‰â€™í•˜ê±°ë‚˜ â€˜ì¹˜í™˜â€™í•˜ëŠ” ê³¼ì •ì„ ë§¤ìš° ê°„í¸í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•˜ë©´ ì½”í¼ìŠ¤ ë‚´ë¶€ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ê¸€ìë“¤ì„ ê·œì¹™ì— ê¸°ë°˜í•˜ì—¬ ì‰½ê²Œ ì œê±°í•  ìˆ˜ ìˆë‹¤. # ê¸¸ì´ê°€ 1~2ì¸ ë‹¨ì–´ë“¤ì„ ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•˜ì—¬ ì‚­ì œ import re text = \"I was wondering if anyone out there could enlighten me on this car.\" shortword = re.compile(r'\\W*\\b\\w{1,2}\\b') print(shortword.sub('', text)) was wondering anyone out there could enlighten this car. ì°¸ê³ ) ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ","date":"2021-08-17","objectID":"/nlp_03/:4:0","tags":["NLP"],"title":"[NLP] 3.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - ì •ì œë° ì •ê·œí™”","uri":"/nlp_03/"},{"categories":["NLP"],"content":"í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ NLP ê³µë¶€","date":"2021-08-15","objectID":"/nlp_02/","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Text Preprocessing Tokenization ì£¼ì–´ì§„ corpusì—ì„œ í† í°(token)ì´ë¼ ë¶ˆë¦¬ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì„ ì§€ì¹­í•œë‹¤ ","date":"2021-08-15","objectID":"/nlp_02/:0:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Word Tokenization í† í°ì˜ ê¸°ì¤€ì„ ë‹¨ì–´(word)ë¡œ í•˜ëŠ” ê²½ìš°, ë‹¨ì–´ í† ë¥¸í™”ë¼ê³  í•œë‹¤. ex) êµ¬ë‘ì (. , ? ; !)ì„ ì œì™¸, ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í† í°í™” Time is an illusion. Lunchtime double so! \u003e \"Time\", \"is\", \"an\", \"illustion\", \"Lunchtime\", \"double\", \"so\" ","date":"2021-08-15","objectID":"/nlp_02/:1:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"ê³ ë ¤ ì‚¬í•­ êµ¬ë‘ì , íŠ¹ìˆ˜ ë¬¸ì ì œì™¸ ì—¬ë¶€ ì„ íƒ ex) 2021/08/15: â€˜/â€˜ê°€ ë‚ ì§œë¥¼ êµ¬ë¶„í•´ ì£¼ëŠ” ì—­í•  ì¤„ì„ë§, ë‹¨ì–´ ë‚´ ë„ì–´ì“°ê¸° ex) New York: â€œNew\"ì™€ â€œYork\"ë¡œ ë¶„ë¦¬ì‹œ ë³¸ë˜ ì˜ë¯¸ê°€ ì—†ì–´ì§ˆ ìˆ˜ ìˆìŒ ","date":"2021-08-15","objectID":"/nlp_02/:1:1","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Penn Treebank Tokenization í‘œì¤€ìœ¼ë¡œ ì“°ì´ëŠ” í† í°í™” ë°©ë²• ì¤‘ í•˜ë‚˜ Rule) í•˜ì´í”ˆ(â€™-â€™)ìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ëŠ” í•˜ë‚˜ë¡œ ìœ ì§€ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼(â€™`â€™)ë¡œ â€œì ‘ì–´\"ê°€ í•¨ê»˜í•˜ëŠ” ë‹¨ì–´ëŠ” ë¶„ë¦¬ from nltk.tokenize import TreebankWordTokenizer tokenizer = TreebankWordTokenizer() txt = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\" print(tokenizer.tokenize(txt)) ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.'] ","date":"2021-08-15","objectID":"/nlp_02/:1:2","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Sentence Tokenization í† í°ì˜ ë‹¨ìœ„ê°€ ë¬¸ì¥(sentence)ì¸ ê²½ìš° corpusë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œëŠ”, corpusê°€ ì–´ë–¤ êµ­ì ì˜ ì–¸ì–´ì¸ì§€, í•´ë‹¹ corpusì—ì„œ íŠ¹ìˆ˜ë¬¸ìë“¤ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ì— ë”°ë¼ ì§ì ‘ ê·œì¹™ì„ ì •ì˜í•  í•„ìš”ê°€ ìˆë‹¤. # NLTKì—ì„œ ì§€ì›í•˜ëŠ” ì˜ì–´ë¬¸ì¥ í† í°í™” - sent_tokenize import nltk nltk.download('punkt') from nltk.tokenize import sent_tokenize sent_tokenize(text) text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\" ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.'] # í•œêµ­ì–´ ë¬¸ì¥ í† í°í™” ë„êµ¬ import kss txt = 'ë”¥ ëŸ¬ë‹ ìì—°ì–´ ì²˜ë¦¬ê°€ ì¬ë¯¸ìˆê¸°ëŠ” í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ë¬¸ì œëŠ” ì˜ì–´ë³´ë‹¤ í•œêµ­ì–´ë¡œ í•  ë•Œ ë„ˆë¬´ ì–´ë ¤ì›Œìš”. ë†ë‹´ì•„ë‹ˆì—ìš”. ì´ì œ í•´ë³´ë©´ ì•Œê±¸ìš”?' print(kss.split_sentences(txt)) ['ë”¥ ëŸ¬ë‹ ìì—°ì–´ ì²˜ë¦¬ê°€ ì¬ë¯¸ìˆê¸°ëŠ” í•©ë‹ˆë‹¤.', 'ê·¸ëŸ°ë° ë¬¸ì œëŠ” ì˜ì–´ë³´ë‹¤ í•œêµ­ì–´ë¡œ í•  ë•Œ ë„ˆë¬´ ì–´ë ¤ì›Œìš”.', 'ë†ë‹´ì•„ë‹ˆì—ìš”.', 'ì´ì œ í•´ë³´ë©´ ì•Œê±¸ìš”?'] ","date":"2021-08-15","objectID":"/nlp_02/:2:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"Binary Classifier ë¬¸ì¥ í† í°í™”ì˜ ì˜ˆì™¸ ì‚¬í•­ì„ ë°œìƒì‹œí‚¤ëŠ” ë§ˆì¹¨í‘œì˜ ì²˜ë¦¬ë¥¼ ìœ„í•´, ì…ë ¥ì— ë”°ë¼ ë‘ ê°œì˜ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ê¸°(binary classifier)ë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. ë‘ ê°œì˜ í´ë˜ìŠ¤ ë§ˆì¹¨í‘œ(.)ê°€ ë‹¨ì–´ì˜ ì¼ë¶€ë¶„(ì•½ì–´)ì¸ ê²½ìš° ë§ˆì¹¨í‘œ(.)ê°€ ë¬¸ì¥ì˜ êµ¬ë¶„ìì¸ ê²½ìš° ","date":"2021-08-15","objectID":"/nlp_02/:3:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"í•œêµ­ì–´ í† í°í™”ì˜ ì–´ë ¤ì›€ êµì°©ì–´ ì–´ê·¼ê³¼ ì ‘ì‚¬ì— ì˜í•´ ë‹¨ì–´ì˜ ê¸°ëŠ¥ì´ ê²°ì •. í˜•íƒœì†Œ(morpheme)ì˜ ê°œë… ì´í•´ê°€ í•„ìˆ˜ì : í˜•íƒœì†Œ í† í°í™” -\u003e í˜•íƒœì†Œ: ëœ»ì„ ê°€ì§„ ê°€ì¥ ì‘ì€ ë§ì˜ ë‹¨ìœ„ ìë¦½ í˜•íƒœì†Œ: ì ‘ì‚¬, ì–´ë¯¸, ì¡°ì‚¬ì™€ ìƒê´€ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœì†Œ ì˜ì¡´ í˜•íƒœì†Œ: ë‹¤ë¥¸ í˜•íƒœì†Œì™€ ê²°í•©í•˜ì—¬ ì‚¬ìš©ë˜ëŠ” í˜•íƒœì†Œ ë„ì–´ì“°ê¸°ê°€ ì§€ì¼œì§€ì§€ ì•ŠëŠ” corpusê°€ ë¹„êµì  ë§ìŒ ","date":"2021-08-15","objectID":"/nlp_02/:4:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"í’ˆì‚¬ íƒœê¹…(Part-of-speech tagging) ë‹¨ì–´ì˜ í‘œê¸°ëŠ” ê°™ìœ¼ë‚˜, í’ˆì‚¬ì— ë”°ë¼ì„œ ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ê¸°ë„ í•¨. ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì œëŒ€ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¨ì–´ê°€ ì–´ë–¤ í’ˆì‚¬ë¡œ ì“°ì˜€ëŠ”ì§€ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì£¼ìš” ì§€í‘œê°€ ë  ìˆ˜ ìˆìŒ. ","date":"2021-08-15","objectID":"/nlp_02/:5:0","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"nltk pos_tag from nltk.tokenize import word_tokenize from nltk.tag import pos_tag text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\" a = word_tokenize(text) pos_tag(a) [('I', 'PRP'), #ì¸ì¹­ëŒ€ëª…ì‚¬ ('am', 'VBP'), #ë™ì‚¬ ('actively', 'RB'), #ë¶€ì‚¬ ('looking', 'VBG'), #í˜„ì¬ë¶€ì‚¬ ('for', 'IN'), #ì „ì¹˜ì‚¬ ('Ph.D.', 'NNP'), #ê³ ìœ ëª…ì‚¬ ('students', 'NNS'), #ë³µìˆ˜í˜•ëª…ì‚¬ ('.', '.'), ('and', 'CC'), #ì ‘ì†ì‚¬ ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), #ê´€ì‚¬ ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')] ","date":"2021-08-15","objectID":"/nlp_02/:5:1","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"KoNLPy í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì„  íŒ¨í‚¤ì§€ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ Okt(Open Korea Text), ë©”ìº…(Mecab), ì½”ëª¨ë€(Komoran), í•œë‚˜ëˆ”(Hannanum), ê¼¬ê¼¬ë§ˆ(Kkma) ì§€ì› í˜•íƒœì†Œ ë¶„ì„ê¸° ë§ˆë‹¤ ì„±ëŠ¥ê³¼ ê²°ê³¼ê°€ ë‹¤ë¥´ê²Œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì—, í•„ìš” ìš©ë„ì— ë”°ë¼ ì–´ë˜ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì ì ˆí•œì§€ íŒë‹¨í•˜ê³  ì‚¬ìš©. #okt from konlpy.tag import Okt okt=Okt() print(okt.morphs(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) print(okt.pos(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) print(okt.nouns(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) ['ì—´ì‹¬íˆ', 'ì½”ë”©', 'í•œ', 'ë‹¹ì‹ ', ',', 'ì—°íœ´', 'ì—ëŠ”', 'ì—¬í–‰', 'ì„', 'ê°€ë´ìš”'] [('ì—´ì‹¬íˆ', 'Adverb'), ('ì½”ë”©', 'Noun'), ('í•œ', 'Josa'), ('ë‹¹ì‹ ', 'Noun'), (',', 'Punctuation'), ('ì—°íœ´', 'Noun'), ('ì—ëŠ”', 'Josa'), ('ì—¬í–‰', 'Noun'), ('ì„', 'Josa'), ('ê°€ë´ìš”', 'Verb')] ['ì½”ë”©', 'ë‹¹ì‹ ', 'ì—°íœ´', 'ì—¬í–‰'] #kkma from konlpy.tag import Kkma kkma=Kkma() print(kkma.morphs(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) print(kkma.pos(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) print(kkma.nouns(\"ì—´ì‹¬íˆ ì½”ë”©í•œ ë‹¹ì‹ , ì—°íœ´ì—ëŠ” ì—¬í–‰ì„ ê°€ë´ìš”\")) ['ì—´ì‹¬íˆ', 'ì½”ë”©', 'í•˜', 'ã„´', 'ë‹¹ì‹ ', ',', 'ì—°íœ´', 'ì—', 'ëŠ”', 'ì—¬í–‰', 'ì„', 'ê°€ë³´', 'ì•„ìš”'] [('ì—´ì‹¬íˆ', 'MAG'), ('ì½”ë”©', 'NNG'), ('í•˜', 'XSV'), ('ã„´', 'ETD'), ('ë‹¹ì‹ ', 'NP'), (',', 'SP'), ('ì—°íœ´', 'NNG'), ('ì—', 'JKM'), ('ëŠ”', 'JX'), ('ì—¬í–‰', 'NNG'), ('ì„', 'JKO'), ('ê°€ë³´', 'VV'), ('ì•„ìš”', 'EFN')] ['ì½”ë”©', 'ë‹¹ì‹ ', 'ì—°íœ´', 'ì—¬í–‰'] ì°¸ê³ ) ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ","date":"2021-08-15","objectID":"/nlp_02/:5:2","tags":["NLP"],"title":"[NLP] 2.í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ - í† í°í™”","uri":"/nlp_02/"},{"categories":["NLP"],"content":"í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ NLP ê³µë¶€","date":"2021-08-15","objectID":"/nlp_01/","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"NLPê¸°ì´ˆ ì‹¤ì œ NLP ê´€ë ¨ í”„ë¡œì íŠ¸ì— ë“¤ì–´ê°€ê¸° ì•ì„œ, NLP ê¸°ì´ˆë¥¼ ë‹¤ì§€ê³ ì í•œë‹¤ ","date":"2021-08-15","objectID":"/nlp_01/:0:0","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ë¨¸ì‹ ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ","date":"2021-08-15","objectID":"/nlp_01/:1:0","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Tensorflow ë¨¸ì‹ ëŸ¬ë‹ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì„ ì§ê´€ì ì´ê³  ì†ì‰½ê²Œ í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ ","date":"2021-08-15","objectID":"/nlp_01/:1:1","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Keras í…ì„œí”Œë¡œìš°ì— ëŒ€í•œ ì¶”ìƒí™”ëœ API ì œê³µ ë°±ì—”ë“œë¡œ í…ì„œí”Œë¡œìš° ì‚¬ìš©, ì¢€ ë” ì‰½ê²Œ ë”¥ ëŸ¬ë‹ì„ í•  ìˆ˜ ìˆë„ë¡ ë„ì›€ ","date":"2021-08-15","objectID":"/nlp_01/:1:2","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Gensim ë¨¸ì‹ ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ íŠ¸í”½ ëª¨ë¸ë§ê³¼ ìì—°ì–´ ì²˜ë¦¬ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ","date":"2021-08-15","objectID":"/nlp_01/:1:3","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Scikit-learn íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ Naive bayse, SVMë“± ë‹¤ì–‘í•œ ë¨¸ì‹  ëŸ¬ë‹ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ ìì²´ ë°ì´í„° ì œê³µ(ì•„ì´ë¦¬ìŠ¤ ë°ì´í„°, ë‹¹ë‡¨ë³‘ ë°ì´í„° ë“±ë“±..) ","date":"2021-08-15","objectID":"/nlp_01/:1:4","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"NLTK ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´ì¬ íŒ¨í‚¤ì§€ ","date":"2021-08-15","objectID":"/nlp_01/:1:5","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"KoNLPy í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ í˜•íƒœì†Œ ë¶„ì„ê¸° íŒ¨í‚¤ì§€ ","date":"2021-08-15","objectID":"/nlp_01/:1:6","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ë°ì´í„° ë¶„ì„ íŒ¨í‚¤ì§€ ","date":"2021-08-15","objectID":"/nlp_01/:2:0","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Pandas íŒŒì´ì¬ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¸ ì¢…ë¥˜ì˜ ë°ì´í„° êµ¬ì¡° ì‚¬ìš© Series 1ì°¨ì› ë°°ì—´ì˜ ê°’(value)ì— ê° ê°’ì— ëŒ€ì‘ë˜ëŠ” ì¸ë±ìŠ¤(index)ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡° import pandas as pd sr = pd.Series([100,200,300,400], index = [\"a\",\"b\",\"c\",\"d\"]) print(sr) a 100 b 200 c 300 d 400 dtype: int64 DataFrame 2ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¥¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬. í–‰ ë°©í–¥ ì¸ë±ìŠ¤ì™€ ì—´ ë°©í–¥ ì¸ë±ìŠ¤ê°€ ì¡´ì¬ -\u003e í–‰ê³¼ ì—´ì„ ê°€ì§€ëŠ” ìë£Œêµ¬ì¡° List, Series, Dict, ndarraysë“±ì„ í†µí•´ ìƒì„±í•  ìˆ˜ ìˆë‹¤. values = [[1,2,3], [4,5,6], [7,8,9]] index = [\"I1\",\"I2\",\"I3\"] columns = [\"C1\",\"C2\",\"C3\"] df = pd.DataFrame(values, index=index, columns=columns) print(df) C1 C2 C3 I1 1 2 3 I2 4 5 6 I3 7 8 9 # ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±í•˜ê¸° data = [ ['1000', 'Steve', 90.72], ['1001', 'James', 78.09], ['1002', 'Doyeon', 98.43], ['1003', 'Jane', 64.19], ['1004', 'Pilwoong', 81.30], ['1005', 'Tony', 99.14], ] df = pd.DataFrame(data) print(df) 0 1 2 0 1000 Steve 90.72 1 1001 James 78.09 2 1002 Doyeon 98.43 3 1003 Jane 64.19 4 1004 Pilwoong 81.30 5 1005 Tony 99.14 data = { 'í•™ë²ˆ' : ['1000', '1001', '1002', '1003', '1004', '1005'], 'ì´ë¦„' : [ 'Steve', 'James', 'Doyeon', 'Jane', 'Pilwoong', 'Tony'], 'ì ìˆ˜': [90.72, 78.09, 98.43, 64.19, 81.30, 99.14]} df = pd.DataFrame(data) print(df) í•™ë²ˆ ì´ë¦„ ì ìˆ˜ 0 1000 Steve 90.72 1 1001 James 78.09 2 1002 Doyeon 98.43 3 1003 Jane 64.19 4 1004 Pilwoong 81.30 5 1005 Tony 99.14 Panel ","date":"2021-08-15","objectID":"/nlp_01/:2:1","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Numpy ìˆ˜ì¹˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ í–‰ë ¬ ìë£Œêµ¬ì¡°ì¸ ndarrayë¥¼ í†µí•´ ì„ í˜• ëŒ€ìˆ˜ ê³„ì‚°ì— ë§ì´ ì‚¬ìš© np.array() ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë°°ì—´ë¡œë¶€í„° ndarrayìƒì„± ì¸ë±ìŠ¤ê°€ í•­ìƒ 0ìœ¼ë¡œ ì‹œì‘í•¨ ndarr1 = np.array([1,2,3],[4,5,6]) print(ndarr1.shape()) (2,3) # 2 x 3 í–‰ë ¬ ndarrayì´ˆê¸°í™” ndarr2 = np.zeros(2,3) # ëª¨ë“  ê°’ì´ 0ì¸ 2x3 í–‰ë ¬ ndarr3 = np.full((2,2), 4) # ëª¨ë“  ê°’ì´ íŠ¹ì • ìƒìˆ˜ê°’ì¸ í–‰ë ¬ ndarr4 = np.eye(3) # ëŒ€ê°ì„  1, ë‚˜ë¨¸ì§€ëŠ” 0ì¸ 3x3 í–‰ë ¬ print(ndarr2) print(ndarr3) [[0,0,0] [0,0,0]] [[4,4] [4,4]] [[1,0,0] [0,1,0] [0,0,1]] np.arange() ì§€ì •í•´ì¤€ ë²”ìœ„ì— ëŒ€í•´ì„œ ë°°ì—´ ìƒì„± numpy.arange(start, stop, stem, dtype) a = np.arange(1,10,2) # 1ë¶€í„° 9ê¹Œì§€ +2 [1,3,5,7,9] reshape() ë°°ì—´ì„ ë‹¤ì°¨ì›ìœ¼ë¡œ ë³€í˜• a = np.array(np.arange(30)).reshape((5,6)) print(a) [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29]] ","date":"2021-08-15","objectID":"/nlp_01/:2:2","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Matplotlib ë°ì´í„°ë¥¼ ì°¨íŠ¸ë‚˜ í”Œë¡¯ìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” íŒ¨í‚¤ì§€ ","date":"2021-08-15","objectID":"/nlp_01/:2:3","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"Machine Learning Workflow ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ \" ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ (ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸) ","date":"2021-08-15","objectID":"/nlp_01/:3:0","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ìˆ˜ì§‘(Acquisition) ë¨¸ì‹  ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ìì—°ì–´ ë°ì´í„°: Corpus(ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ì˜ ì§‘í•©) ","date":"2021-08-15","objectID":"/nlp_01/:3:1","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ì ê²€ ë° íƒìƒ‰(Inspection and exploration) ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì ê²€, íƒìƒ‰ ë¨¸ì‹ ëŸ¬ë‹ ì ìš©ì„ ìœ„í•´ì„œ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ì •ì œí• ì§€ íŒŒì•… íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(Exploratory Data Analysis, EDA)ë‹¨ê³„ë¼ê³ ë„ ë¶€ë¦„ ","date":"2021-08-15","objectID":"/nlp_01/:3:2","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ì „ì²˜ë¦¬ ë° ì •ì œ(Preprocessing and Cleaning) í† í°í™”, ì •ì œ, ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±° ë‹¨ê³„ ","date":"2021-08-15","objectID":"/nlp_01/:3:3","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ëª¨ë¸ë§ ë° í›ˆë ¨(Modeling and Training) ë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ë‹¨ê³„ ì „ì²˜ë¦¬ê°€ ì™„ë£Œëœ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê¸°ê³„ì— í•™ìŠµ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°, ëª¨ë“  ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ì§€ ì•Šê³ , í›ˆë ¨ìš© ë°ì´í„°ë§Œ í•™ìŠµì‹œì¼œ ê³¼ì í•©(Overfitting)ìƒí™©ì„ í”¼í•œë‹¤. ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ \" ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ (ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸) ","date":"2021-08-15","objectID":"/nlp_01/:3:4","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"í‰ê°€(Evaluation) í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ í‰ê°€ ê¸°ê³„ê°€ ì˜ˆì¸¡í•œ ë°ì´í„°ê°€ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì˜ ì‹¤ì œ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ ì¸¡ì • ","date":"2021-08-15","objectID":"/nlp_01/:3:5","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"},{"categories":["NLP"],"content":"ë°°í¬(Deployment) ì™„ì„±ëœ ëª¨ë¸ì„ ë°°í¬ ì°¸ê³ ) ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸ ","date":"2021-08-15","objectID":"/nlp_01/:3:6","tags":["NLP"],"title":"[NLP] 1.í”„ë ˆì„ì›Œí¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬","uri":"/nlp_01/"}]